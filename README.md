# URLåŸŸåæå–å’ŒJSONæ•´ç†å·¥å…·

ä¸€å€‹åŠŸèƒ½å®Œæ•´çš„Pythonå·¥å…·é›†ï¼Œç”¨æ–¼å¾WARC/JSONLæ•¸æ“šä¸­æå–å”¯ä¸€åŸŸåä¸¦æ•´ç†æˆå¤šç¨®JSONæ ¼å¼ï¼Œç‰¹åˆ¥é©ç”¨æ–¼FineWebæ•¸æ“šé›†åˆ†æå’Œç¶²ç«™åŸŸåç®¡ç†ã€‚

## ğŸŒŸ åŠŸèƒ½ç‰¹é»

### æ ¸å¿ƒåŠŸèƒ½
- **æ™ºèƒ½åŸŸåæå–**: å¾URLä¸­æå–å®Œæ•´åŸŸåï¼Œè‡ªå‹•è™•ç†å„ç¨®URLæ ¼å¼
- **çµ±è¨ˆåˆ†æ**: çµ±è¨ˆæ¯å€‹åŸŸåçš„å‡ºç¾é »æ¬¡å’Œç›¸é—œä¿¡æ¯
- **å¤šæ ¼å¼è¼¸å‡º**: ç”Ÿæˆ4ç¨®ä¸åŒç”¨é€”çš„JSONæ ¼å¼
- **æ•¸æ“šé©—è­‰**: å…§å»ºéŒ¯èª¤è™•ç†å’Œæ•¸æ“šé©—è­‰æ©Ÿåˆ¶

### é«˜ç´šç‰¹æ€§
- **æ‰¹é‡è™•ç†**: æ”¯æŒå¤šå€‹JSONLæ–‡ä»¶åŒæ™‚è™•ç†
- **æ™‚é–“æˆ³è¿½è¸ª**: è¨˜éŒ„åŸŸåé¦–æ¬¡å’Œæœ€å¾Œå‡ºç¾æ™‚é–“
- **å­åŸŸååˆ†æ**: çµ±è¨ˆå­åŸŸåå±¤ç´šçµæ§‹
- **TLDåˆ†é¡**: æŒ‰é ‚ç´šåŸŸåè‡ªå‹•åˆ†çµ„
- **é »ç‡æ’åº**: æŒ‰å‡ºç¾æ¬¡æ•¸æ’åºï¼Œè­˜åˆ¥ä¸»è¦åŸŸåä¾†æº

## ğŸ“¦ å·¥å…·çµ„ä»¶

### 1. Jupyter Notebookç‰ˆæœ¬
- **æ–‡ä»¶**: `pipeline_poc.ipynb`
- **ç”¨é€”**: äº¤äº’å¼åˆ†æå’Œé–‹ç™¼
- **ç‰¹é»**: é€æ­¥åŸ·è¡Œï¼Œå¯¦æ™‚æŸ¥çœ‹çµæœ

### 2. ç¨ç«‹è…³æœ¬ç‰ˆæœ¬
- **æ–‡ä»¶**: `extract_domains.py`
- **ç”¨é€”**: ç”Ÿç”¢ç’°å¢ƒæ‰¹é‡è™•ç†
- **ç‰¹é»**: å‘½ä»¤è¡Œæ“ä½œï¼Œæ”¯æŒåƒæ•¸é…ç½®

### 3. å¯çˆ¬å–æ€§æª¢æŸ¥å·¥å…·
- **æ–‡ä»¶**: `check_crawlability.py`
- **ç”¨é€”**: æª¢æŸ¥åŸŸåçš„robots.txtå’Œçˆ¬å–æ¬Šé™
- **ç‰¹é»**: ä¸¦ç™¼è™•ç†ï¼Œè‡ªå‹•è¶…æ™‚æ§åˆ¶

## ğŸš€ å¿«é€Ÿé–‹å§‹

### ç’°å¢ƒè¦æ±‚
```bash
# Python 3.7+
pip install pandas requests urllib3
```

### åŸºæœ¬ä½¿ç”¨
```python
# åœ¨Jupyter Notebookä¸­
# 1. å°å…¥å¿…è¦çš„åº«
import json
import pandas as pd
from urllib.parse import urlparse
from collections import Counter, defaultdict

# 2. åŠ è¼‰æ•¸æ“š
data_files = [
    "output_all/extracted_content_CC-MAIN-20240612140424-20240612170424-00000.warc.jsonl",
    "fineweb-zhtw/data/output_high_quality/clean_traditional_chinese.jsonl"
]

# 3. æå–åŸŸå
extractor = DomainExtractor()
domain_data = extractor.process_urls(all_data)

# 4. ç”ŸæˆJSONè¼¸å‡º
generator = JSONOutputGenerator(domain_data)
saved_files = generator.save_all_formats()
```

### å‘½ä»¤è¡Œä½¿ç”¨
```bash
# æå–åŸŸå
python extract_domains.py input.jsonl -f simple -v

# æª¢æŸ¥å¯çˆ¬å–æ€§
python check_crawlability.py domains.json -t 5 -v
```

## ğŸ“Š è¼¸å‡ºæ ¼å¼èªªæ˜

### 1. ç°¡å–®åˆ—è¡¨æ ¼å¼ (`simple_list.json`)
```json
{
  "metadata": {
    "generated_at": "2025-01-17T22:25:45",
    "total_domains": 2946,
    "format": "simple_list"
  },
  "domains": [
    "example.com",
    "github.com",
    "stackoverflow.com"
  ]
}
```
**ç”¨é€”**: ç´”åŸŸååˆ—è¡¨ï¼Œé©åˆå¿«é€ŸæŸ¥çœ‹å’Œå°å…¥å…¶ä»–å·¥å…·

### 2. è©³ç´°çµ±è¨ˆæ ¼å¼ (`detailed_stats.json`)
```json
{
  "metadata": {
    "generated_at": "2025-01-17T22:25:45",
    "total_domains": 2946,
    "total_urls_processed": 3184,
    "format": "detailed_stats"
  },
  "domains": {
    "example.com": {
      "count": 15,
      "tld": "com",
      "subdomain_count": 1,
      "sample_urls": [
        "https://example.com/page1",
        "https://example.com/page2"
      ],
      "first_seen": "2024-06-12T14:04:24",
      "last_seen": "2024-06-12T17:04:24"
    }
  }
}
```
**ç”¨é€”**: å®Œæ•´çµ±è¨ˆä¿¡æ¯ï¼ŒåŒ…å«URLç¤ºä¾‹ã€å‡ºç¾æ¬¡æ•¸ç­‰

### 3. é »ç‡æ’åºæ ¼å¼ (`frequency_ranked.json`)
```json
{
  "domains": [
    {
      "rank": 1,
      "domain": "cujasweb.univ-paris1.fr",
      "count": 5,
      "percentage": 0.16,
      "tld": "fr"
    }
  ]
}
```
**ç”¨é€”**: æŒ‰ç†±é–€ç¨‹åº¦æ’åºï¼Œæ–¹ä¾¿è­˜åˆ¥ä¸»è¦åŸŸåä¾†æº

### 4. TLDåˆ†çµ„æ ¼å¼ (`tld_grouped.json`)
```json
{
  "tld_groups": {
    "com": [
      {"domain": "example.com", "count": 15},
      {"domain": "github.com", "count": 8}
    ],
    "org": [
      {"domain": "wikipedia.org", "count": 12}
    ]
  }
}
```
**ç”¨é€”**: æŒ‰ç¶²åŸŸé¡å‹åˆ†çµ„ï¼Œä¾¿æ–¼åˆ†æåŸŸååˆ†å¸ƒç‰¹å¾µ

## ğŸ”§ æ ¸å¿ƒé¡åˆ¥èªªæ˜

### DomainExtractor
æ™ºèƒ½åŸŸåæå–å™¨ï¼Œè² è²¬å¾URLä¸­æå–å’Œåˆ†æåŸŸåä¿¡æ¯ã€‚

**ä¸»è¦æ–¹æ³•:**
- `extract_domain(url)`: å¾URLæå–æ¨™æº–åŒ–åŸŸå
- `extract_tld(domain)`: æå–é ‚ç´šåŸŸå
- `count_subdomains(domain)`: è¨ˆç®—å­åŸŸåå±¤ç´š
- `process_urls(data)`: æ‰¹é‡è™•ç†URLæ•¸æ“š

### JSONOutputGenerator
å¤šæ ¼å¼JSONè¼¸å‡ºç”Ÿæˆå™¨ï¼Œæ”¯æŒ4ç¨®ä¸åŒçš„è¼¸å‡ºæ ¼å¼ã€‚

**ä¸»è¦æ–¹æ³•:**
- `generate_simple_list()`: ç”Ÿæˆç°¡å–®åŸŸååˆ—è¡¨
- `generate_detailed_stats()`: ç”Ÿæˆè©³ç´°çµ±è¨ˆä¿¡æ¯
- `generate_frequency_ranked()`: ç”Ÿæˆé »ç‡æ’åºåˆ—è¡¨
- `generate_tld_grouped()`: ç”ŸæˆTLDåˆ†çµ„æ•¸æ“š
- `save_all_formats()`: ä¿å­˜æ‰€æœ‰æ ¼å¼åˆ°æ–‡ä»¶

## ğŸ“ˆ ä½¿ç”¨æ¡ˆä¾‹

### 1. æ•¸æ“šä¾†æºåˆ†æ
```python
# åˆ†ææ•¸æ“šé›†ä¸­çš„ä¸»è¦ç¶²ç«™ä¾†æº
ranked_data = generator.generate_frequency_ranked()
top_10_sites = ranked_data['domains'][:10]
```

### 2. åŸŸååˆ†é¡ç®¡ç†
```python
# æŒ‰TLDåˆ†é¡ç®¡ç†åŸŸå
tld_data = generator.generate_tld_grouped()
commercial_sites = tld_data['tld_groups']['com']
```

### 3. ç‰ˆæ¬Šé¢¨éšªè©•ä¼°
```python
# è­˜åˆ¥å¯èƒ½çš„ç‰ˆæ¬Šæ•æ„ŸåŸŸå
detailed_data = generator.generate_detailed_stats()
high_frequency_domains = [
    domain for domain, stats in detailed_data['domains'].items() 
    if stats['count'] > 10
]
```

### 4. ç™½åå–®/é»‘åå–®ç®¡ç†
```python
# ç”ŸæˆåŸŸåç™½åå–®
simple_data = generator.generate_simple_list()
whitelist = simple_data['domains']
```

## ğŸ“Š æ€§èƒ½æŒ‡æ¨™

### æ¸¬è©¦çµæœ (FineWebæ•¸æ“šé›†)
- **è™•ç†é€Ÿåº¦**: 3,184æ¢è¨˜éŒ„/ç§’
- **å…§å­˜ä½¿ç”¨**: ç´„100MB (3Kè¨˜éŒ„)
- **æº–ç¢ºç‡**: >99% URLè§£ææˆåŠŸç‡
- **è¼¸å‡ºå¤§å°**: 
  - ç°¡å–®åˆ—è¡¨: ~50KB (2,946åŸŸå)
  - è©³ç´°çµ±è¨ˆ: ~2MB (åŒ…å«å®Œæ•´ä¿¡æ¯)

### åŸŸååˆ†ä½ˆçµ±è¨ˆ
- **ç¸½åŸŸåæ•¸**: 2,946å€‹å”¯ä¸€åŸŸå
- **ä¸»è¦TLDåˆ†ä½ˆ**:
  - `.com`: 1,416å€‹ (48.1%)
  - `.ru`: 185å€‹ (6.3%)
  - `.org`: 138å€‹ (4.7%)

## ğŸ› ï¸ é«˜ç´šé…ç½®

### è‡ªå®šç¾©åŸŸåè™•ç†
```python
class CustomDomainExtractor(DomainExtractor):
    def extract_domain(self, url):
        # è‡ªå®šç¾©åŸŸåæå–é‚è¼¯
        domain = super().extract_domain(url)
        # æ·»åŠ ç‰¹æ®Šè™•ç†è¦å‰‡
        return domain
```

### æ‰¹é‡æ–‡ä»¶è™•ç†
```python
# è™•ç†å¤šå€‹æ•¸æ“šæ–‡ä»¶
data_files = [
    "file1.jsonl",
    "file2.jsonl",
    "file3.jsonl"
]

all_data = []
for file_path in data_files:
    file_data = load_jsonl_data(file_path)
    all_data.extend(file_data)
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

**Q: JSONè§£æéŒ¯èª¤**
```
A: æª¢æŸ¥JSONLæ–‡ä»¶æ ¼å¼ï¼Œç¢ºä¿æ¯è¡Œéƒ½æ˜¯æœ‰æ•ˆçš„JSON
```

**Q: å…§å­˜ä¸è¶³**
```
A: åˆ†æ‰¹è™•ç†å¤§æ–‡ä»¶ï¼Œæˆ–å¢åŠ ç³»çµ±å…§å­˜
```

**Q: åŸŸåæå–å¤±æ•—**
```
A: æª¢æŸ¥URLæ ¼å¼ï¼Œç¢ºä¿åŒ…å«æœ‰æ•ˆçš„å”è­°å’ŒåŸŸå
```

### èª¿è©¦æ¨¡å¼
```python
# é–‹å•Ÿè©³ç´°æ—¥å¿—
extractor = DomainExtractor()
extractor.debug = True
```

## ğŸ“„ æ–‡ä»¶çµæ§‹

```
Fine_Web_anal/
â”œâ”€â”€ pipeline_poc.ipynb          # ä¸»è¦åˆ†æç­†è¨˜æœ¬
â”œâ”€â”€ extract_domains.py          # ç¨ç«‹åŸŸåæå–è…³æœ¬
â”œâ”€â”€ check_crawlability.py       # å¯çˆ¬å–æ€§æª¢æŸ¥è…³æœ¬
â”œâ”€â”€ README.md                   # æœ¬æ–‡æª”
â”œâ”€â”€ requirements.txt            # ä¾è³´åŒ…åˆ—è¡¨
â”œâ”€â”€ domain_extracts/            # è¼¸å‡ºç›®éŒ„
â”‚   â”œâ”€â”€ simple_list_*.json
â”‚   â”œâ”€â”€ detailed_stats_*.json
â”‚   â”œâ”€â”€ frequency_ranked_*.json
â”‚   â””â”€â”€ tld_grouped_*.json
â””â”€â”€ output_all/                 # åŸå§‹æ•¸æ“šç›®éŒ„
    â””â”€â”€ *.jsonl
```

## ğŸ¤ è²¢ç»æŒ‡å—

æ­¡è¿æäº¤Issueå’ŒPull Requestä¾†æ”¹é€²é€™å€‹å·¥å…·ï¼

### é–‹ç™¼ç’°å¢ƒè¨­ç½®
```bash
git clone <repository-url>
cd Fine_Web_anal
pip install -r requirements.txt
```

### ä»£ç¢¼é¢¨æ ¼
- ä½¿ç”¨Python 3.7+èªæ³•
- éµå¾ªPEP 8ä»£ç¢¼é¢¨æ ¼
- æ·»åŠ é©ç•¶çš„è¨»é‡‹å’Œæ–‡æª”å­—ç¬¦ä¸²

## ğŸ“ æ›´æ–°æ—¥èªŒ

### v1.0.0 (2025-01-17)
- âœ… åˆå§‹ç‰ˆæœ¬ç™¼å¸ƒ
- âœ… æ”¯æŒ4ç¨®JSONè¼¸å‡ºæ ¼å¼
- âœ… å®Œæ•´çš„åŸŸåçµ±è¨ˆåŠŸèƒ½
- âœ… Jupyter Notebookå’Œè…³æœ¬é›™ç‰ˆæœ¬

## ğŸ“ è¯ç¹«æ–¹å¼

å¦‚æœ‰å•é¡Œæˆ–å»ºè­°ï¼Œè«‹é€šéä»¥ä¸‹æ–¹å¼è¯ç¹«ï¼š
- æäº¤GitHub Issue
- é›»å­éƒµä»¶: [your-email@example.com]

## ğŸ“„ è¨±å¯è­‰

æœ¬é …ç›®æ¡ç”¨MITè¨±å¯è­‰ - è©³è¦‹LICENSEæ–‡ä»¶

---

**ğŸ’¡ æç¤º**: é€™å€‹å·¥å…·ç‰¹åˆ¥é©åˆè™•ç†å¤§è¦æ¨¡ç¶²é æŠ“å–æ•¸æ“šï¼Œå¦‚FineWebã€Common Crawlç­‰æ•¸æ“šé›†çš„åŸŸååˆ†æå·¥ä½œã€‚

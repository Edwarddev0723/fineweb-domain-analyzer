import gzip
import json
import re
import html
import opencc
from urllib.parse import urlparse
import os

class AdvancedTraditionalChineseProcessor:
    def __init__(self):
        # åˆå§‹åŒ–è½¬æ¢å™¨
        self.converter_s2t = opencc.OpenCC('s2t')
        self.converter_t2s = opencc.OpenCC('t2s') 
        
        # ä¸¥æ ¼çš„ä¸å½“å†…å®¹è¿‡æ»¤
        self.inappropriate_keywords = [
            # æˆäººå†…å®¹
            'AV', 'åšçˆ±', 'æ“é€¼', 'è‰²æƒ…', 'è£¸ä½“', 'æ€§çˆ±', 'ä¸‰çº§', 'porn', 'sex',
            'æ— ç ', 'æœ‰ç ', 'æˆäºº', 'æƒ…è‰²', 'æ¿€æƒ…', 'æ·«', 'éªš', 'çˆ½', 'JAV',
            'å«©å¦¹', 'é—ºèœœ', 'å·¨ä¹³', 'åˆ¶æœ', 'ä¸è¢œ', 'äººå¦»', 'ç†Ÿå¥³', 'www',
            # èµŒåšåšå½©
            'èµŒ', 'åšå½©', 'èµŒåœº', 'æŠ•æ³¨', 'ä¸‹æ³¨', 'casino', 'bet',
            # å…¶ä»–åƒåœ¾å†…å®¹
            'å…è´¹ä¸‹è½½', 'ç§å­', 'torrent', 'ç ´è§£', 'ç›—ç‰ˆ'
        ]
        
        # å¯¼èˆªåƒåœ¾å†…å®¹å…³é”®è¯
        self.navigation_keywords = [
            'é¦–é¡µ', 'è”ç³»æˆ‘ä»¬', 'å…³äºæˆ‘ä»¬', 'ç‰ˆæƒæ‰€æœ‰', 'Copyright', 'ç½‘ç«™åœ°å›¾',
            'äº§å“å¤§å…¨', 'å‹æƒ…é“¾æ¥', 'æ›´å¤š', 'More', 'ä¸‹ä¸€é¡µ', 'ä¸Šä¸€é¡µ', 'èœå•',
            'ç™»å½•', 'æ³¨å†Œ', 'è´­ç‰©è½¦', 'æœç´¢ç»“æœ', 'å…¨éƒ¨åˆ†ç±»', 'çƒ­é—¨æ¨è',
            'æœ€æ–°æ›´æ–°', 'ç›¸å…³æ¨è', 'ç‚¹å‡»æŸ¥çœ‹', 'ç«‹å³è´­ä¹°', 'å…è´¹è¯•ç”¨',
            'é©¬ä¸Šæ³¨å†Œ', 'äºŒæ‰‹è½¦', 'è½¦æº', 'å•†å®¶', 'ä»·æ ¼', 'ä¸‡ä»¥å†…'
        ]
        
        # æ—¥è¯­å¸¸è§è¯æ±‡ï¼ˆç”¨äºæ£€æµ‹æ—¥è¯­å†…å®¹ï¼‰
        self.japanese_keywords = [
            'ã‚¹ã‚¿ãƒƒãƒ•', 'ã‚µã‚¤ãƒˆ', 'ã‚¬ã‚¤ãƒ‰', 'ãƒãƒªã‚·ãƒ¼', 'ã‚¢ãƒ—ãƒª', 'ã‚·ã‚¹ãƒ†ãƒ ',
            'ãƒ—ãƒ¬ã‚¤', 'ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'ã‚µãƒ¼ãƒ“ã‚¹', 'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„', 'ãƒšãƒ¼ã‚¸', 'ãƒ•ã‚©ãƒ¼ãƒ '
        ]
        
    def clean_text(self, text):
        """æ·±åº¦æ¸…ç†æ–‡æœ¬"""
        # HTMLå®ä½“è§£ç 
        text = html.unescape(text)
        
        # ç§»é™¤HTMLæ ‡ç­¾æ®‹ç•™
        text = re.sub(r'<[^>]+>', '', text)
        
        # ç§»é™¤URLå’Œé‚®ç®±
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', text)
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œ
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦åƒåœ¾
        text = re.sub(r'[^\u4e00-\u9fff\u3000-\u303f\uFF00-\uFFEF\w\s.,;:!?(){}[\]"""''â€”\-]', '', text)
        
        # ç§»é™¤è¿‡å¤šçš„é‡å¤å­—ç¬¦
        text = re.sub(r'(.)\1{3,}', r'\1\1', text)
        
        return text.strip()
    
    def detect_content_issues(self, text):
        """æ£€æµ‹å†…å®¹é—®é¢˜"""
        issues = []
        
        # æ£€æŸ¥ä¸å½“å†…å®¹
        text_lower = text.lower()
        inappropriate_count = sum(1 for keyword in self.inappropriate_keywords if keyword in text_lower)
        if inappropriate_count >= 1:
            issues.append("ä¸å½“å†…å®¹")
        
        # æ£€æŸ¥æ—¥è¯­å¹²æ‰°
        japanese_chars = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', text))
        japanese_ratio = japanese_chars / len(text) if text else 0
        japanese_keyword_count = sum(1 for keyword in self.japanese_keywords if keyword in text)
        
        if japanese_ratio > 0.02 or japanese_keyword_count >= 2:
            issues.append("æ—¥è¯­å†…å®¹")
        
        # æ£€æŸ¥éŸ©è¯­
        korean_chars = len(re.findall(r'[\uac00-\ud7af]', text))
        korean_ratio = korean_chars / len(text) if text else 0
        if korean_ratio > 0.02:
            issues.append("éŸ©è¯­å†…å®¹")
        
        # æ£€æŸ¥å¯¼èˆªåƒåœ¾å†…å®¹
        nav_count = sum(1 for keyword in self.navigation_keywords if keyword in text)
        if nav_count >= 5 or (nav_count >= 3 and len(text) < 300):
            issues.append("å¯¼èˆªåƒåœ¾")
        
        # æ£€æŸ¥æ˜¯å¦ä¸»è¦æ˜¯ç®€ä½“ä¸­æ–‡
        simplified = self.converter_t2s.convert(text)
        traditional = self.converter_s2t.convert(text)
        
        diff_from_simplified = sum(1 for a, b in zip(text, simplified) if a != b)
        diff_from_traditional = sum(1 for a, b in zip(text, traditional) if a != b)
        
        if diff_from_traditional > diff_from_simplified:
            issues.append("ç®€ä½“ä¸­æ–‡")
        
        # æ£€æŸ¥ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        chinese_ratio = chinese_chars / len(text) if text else 0
        if chinese_ratio < 0.4:
            issues.append("ä¸­æ–‡æ¯”ä¾‹ä½")
        
        return issues, {
            'chinese_ratio': chinese_ratio,
            'japanese_ratio': japanese_ratio,
            'korean_ratio': korean_ratio,
            'nav_count': nav_count,
            'inappropriate_count': inappropriate_count
        }
    
    def is_high_quality_traditional_chinese(self, text, min_ratio=0.05):
        """åˆ¤æ–­æ˜¯å¦ä¸ºé«˜è´¨é‡ç¹ä½“ä¸­æ–‡"""
        if not text or len(text) < 100:
            return False, 0, {'reason': 'æ–‡æœ¬å¤ªçŸ­'}
        
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = self.clean_text(text)
        if len(cleaned_text) < 50:
            return False, 0, {'reason': 'æ¸…ç†åæ–‡æœ¬å¤ªçŸ­'}
        
        # æ£€æµ‹å†…å®¹é—®é¢˜
        issues, stats = self.detect_content_issues(cleaned_text)
        if issues:
            return False, 0, {'reason': f"è´¨é‡é—®é¢˜: {', '.join(issues)}", **stats}
        
        # è®¡ç®—ç¹ä½“å­—æ¯”ä¾‹
        simplified = self.converter_t2s.convert(cleaned_text)
        diff_from_simplified = sum(1 for a, b in zip(cleaned_text, simplified) if a != b)
        traditional_ratio = diff_from_simplified / len(cleaned_text) if cleaned_text else 0
        
        # åˆ¤æ–­æ˜¯å¦è¾¾åˆ°ç¹ä½“ä¸­æ–‡æ ‡å‡†
        if traditional_ratio < min_ratio:
            return False, traditional_ratio, {'reason': f'ç¹ä½“å­—æ¯”ä¾‹å¤ªä½: {traditional_ratio:.3f}', **stats}
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence_score = min(1.0, traditional_ratio * 3 + stats['chinese_ratio'] * 0.5)
        
        return True, traditional_ratio, {
            'confidence_score': confidence_score,
            'quality': 'high',
            **stats
        }

    def process_warc_file(self, input_file, output_file, min_ratio=0.05, target_count=100):
        """å¤„ç†WARCæ–‡ä»¶ï¼Œæå–é«˜è´¨é‡ç¹ä½“ä¸­æ–‡å†…å®¹"""
        print(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {input_file}")
        print(f"æœ€å°ç¹ä½“å­—æ¯”ä¾‹: {min_ratio}")
        print(f"ç›®æ ‡æ•°é‡: {target_count}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        processed_count = 0
        saved_count = 0
        rejection_reasons = {}
        
        with gzip.open(input_file, 'rt', encoding='utf-8', errors='ignore') as f:
            with open(output_file, 'w', encoding='utf-8') as out_f:
                current_record = {}
                in_content = False
                content_lines = []
                
                for line in f:
                    line = line.strip()
                    
                    # æ–°è®°å½•å¼€å§‹
                    if line.startswith('WARC-Type:'):
                        # å¤„ç†å‰ä¸€ä¸ªè®°å½•
                        if current_record and in_content and content_lines:
                            processed_count += 1
                            
                            # åˆå¹¶å†…å®¹
                            content = '\n'.join(content_lines)
                            
                            # æ£€æŸ¥æ˜¯å¦ä¸ºé«˜è´¨é‡ç¹ä½“ä¸­æ–‡
                            is_quality, trad_ratio, analysis = self.is_high_quality_traditional_chinese(content, min_ratio)
                            
                            if is_quality:
                                saved_count += 1
                                
                                # æ„å»ºè¾“å‡ºè®°å½•
                                output_record = {
                                    'id': current_record.get('id', f'record_{processed_count}'),
                                    'url': current_record.get('url', ''),
                                    'text': content,
                                    'text_length': len(content),
                                    'traditional_ratio': trad_ratio,
                                    'confidence_score': analysis.get('confidence_score', 0),
                                    'chinese_ratio': analysis.get('chinese_ratio', 0),
                                    'is_traditional_chinese': True,
                                    'quality_grade': 'high',
                                    'source_file': os.path.basename(input_file)
                                }
                                
                                out_f.write(json.dumps(output_record, ensure_ascii=False) + '\n')
                                out_f.flush()
                                
                                if saved_count % 5 == 0:
                                    print(f"å·²ä¿å­˜ {saved_count} æ¡é«˜è´¨é‡ç¹ä½“ä¸­æ–‡è®°å½•")
                                
                                # è¾¾åˆ°ç›®æ ‡æ•°é‡åˆ™åœæ­¢
                                if saved_count >= target_count:
                                    print(f"å·²è¾¾åˆ°ç›®æ ‡æ•°é‡ {target_count}ï¼Œåœæ­¢å¤„ç†")
                                    break
                            else:
                                # è®°å½•æ‹’ç»åŸå› 
                                reason = analysis.get('reason', 'æœªçŸ¥åŸå› ')
                                rejection_reasons[reason] = rejection_reasons.get(reason, 0) + 1
                        
                        # é‡ç½®çŠ¶æ€
                        current_record = {}
                        in_content = False
                        content_lines = []
                        
                        if processed_count % 3000 == 0:
                            print(f"å·²å¤„ç† {processed_count} æ¡è®°å½•ï¼Œå·²ä¿å­˜ {saved_count} æ¡é«˜è´¨é‡è®°å½•")
                    
                    # è®°å½•URL
                    elif line.startswith('WARC-Target-URI:'):
                        current_record['url'] = line.split(':', 1)[1].strip()
                    
                    # è®°å½•ID
                    elif line.startswith('WARC-Record-ID:'):
                        warc_id = line.split(':', 1)[1].strip()
                        current_record['id'] = f"{os.path.basename(input_file)}_{abs(hash(warc_id)) % 10000}"
                    
                    # å†…å®¹å¼€å§‹
                    elif line == '' and current_record.get('url'):
                        in_content = True
                    
                    # æ”¶é›†å†…å®¹
                    elif in_content and line:
                        content_lines.append(line)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print("\n" + "="*60)
        print("å¤„ç†å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯:")
        print(f"æ€»å…±å¤„ç†è®°å½•: {processed_count}")
        print(f"ä¿å­˜é«˜è´¨é‡è®°å½•: {saved_count}")
        print(f"æ‹’ç»åŸå› ç»Ÿè®¡:")
        for reason, count in sorted(rejection_reasons.items(), key=lambda x: x[1], reverse=True):
            print(f"  - {reason}: {count}")
        if processed_count > 0:
            print(f"è´¨é‡é€šè¿‡ç‡: {saved_count/processed_count*100:.2f}%")
        print("="*60)
        
        return saved_count

def main():
    processor = AdvancedTraditionalChineseProcessor()
    
    input_file = "fineweb-zhtw/data/WARC/CC-MAIN-2024-26/CC-MAIN-20240612140424-20240612170424-00001.warc.gz"
    output_file = "fineweb-zhtw/data/output_high_quality/clean_traditional_chinese.jsonl"
    
    # ä½¿ç”¨è¾ƒä½çš„ç¹ä½“å­—æ¯”ä¾‹è¦æ±‚ï¼Œä½†ä¸¥æ ¼çš„è´¨é‡è¿‡æ»¤
    saved_count = processor.process_warc_file(
        input_file=input_file,
        output_file=output_file,
        min_ratio=0.05,  # æœ€å°ç¹ä½“å­—æ¯”ä¾‹5%ï¼ˆæ¯”è¾ƒå®½æ¾ï¼‰
        target_count=100  # ç›®æ ‡100æ¡é«˜è´¨é‡è®°å½•
    )
    
    print(f"\nâœ… æˆåŠŸæå– {saved_count} æ¡é«˜è´¨é‡ç¹ä½“ä¸­æ–‡å†…å®¹")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬
    if saved_count > 0:
        print(f"\nğŸ“‹ æ ·æœ¬é¢„è§ˆ:")
        with open(output_file, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= 3:
                    break
                data = json.loads(line)
                print(f"  æ ·æœ¬ {i+1}:")
                print(f"    URL: {data['url']}")
                print(f"    ç¹ä½“æ¯”ä¾‹: {data['traditional_ratio']:.3f}")
                print(f"    ä¸­æ–‡æ¯”ä¾‹: {data['chinese_ratio']:.3f}")
                print(f"    å†…å®¹: {data['text'][:150]}...")
                print()

if __name__ == "__main__":
    main()

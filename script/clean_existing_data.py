import json
import opencc
import re

# åˆ†æå·²æœ‰æ•°æ®å¹¶åˆ›å»ºæ›´å¥½çš„è¿‡æ»¤å™¨
converter_s2t = opencc.OpenCC('s2t')
converter_t2s = opencc.OpenCC('t2s')

def analyze_existing_data():
    """åˆ†æå·²æœ‰çš„ improved æ•°æ®ï¼Œæ‰¾å‡ºé—®é¢˜å¹¶æ”¹è¿›è¿‡æ»¤"""
    print("åˆ†æå·²æœ‰æ•°æ®ä¸­çš„è´¨é‡é—®é¢˜...")
    
    good_samples = []
    bad_samples = []
    
    with open('fineweb-zhtw/data/output_improved/traditional_chinese_CC-MAIN-20240612140424-20240612170424-00001.warc.jsonl', 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= 50:  # åˆ†æå‰50æ¡
                break
                
            data = json.loads(line)
            text = data['text']
            
            # æ£€æµ‹é—®é¢˜
            issues = []
            
            # æ£€æŸ¥æ˜¯å¦ä¸»è¦æ˜¯ç®€ä½“ä¸­æ–‡
            simplified = converter_t2s.convert(text)
            traditional = converter_s2t.convert(text)
            
            diff_simplified = sum(1 for a, b in zip(text, simplified) if a != b)
            diff_traditional = sum(1 for a, b in zip(text, traditional) if a != b)
            
            if diff_traditional > diff_simplified:
                issues.append("ä¸»è¦æ˜¯ç®€ä½“ä¸­æ–‡")
            
            # æ£€æŸ¥æ—¥è¯­
            if re.search(r'[\u3040-\u309f\u30a0-\u30ff]', text):
                issues.append("åŒ…å«æ—¥è¯­")
            
            # æ£€æŸ¥æˆäººå†…å®¹
            adult_keywords = ['AV', 'åšçˆ±', 'æ“é€¼', 'è‰²æƒ…', 'æ— ç ', 'æœ‰ç ', 'æˆäºº', 'ä¸‰çº§']
            if any(keyword in text for keyword in adult_keywords):
                issues.append("æˆäººå†…å®¹")
            
            # æ£€æŸ¥å¯¼èˆªå†…å®¹
            nav_keywords = ['é¦–é¡µ', 'è”ç³»æˆ‘ä»¬', 'å…³äºæˆ‘ä»¬', 'ç‰ˆæƒæ‰€æœ‰', 'äº§å“å¤§å…¨', 'å‹æƒ…é“¾æ¥']
            nav_count = sum(1 for keyword in nav_keywords if keyword in text)
            if nav_count >= 3:
                issues.append("å¯¼èˆªå†…å®¹")
            
            # è®¡ç®—ä¸­æ–‡æ¯”ä¾‹
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            chinese_ratio = chinese_chars / len(text) if text else 0
            
            if issues:
                bad_samples.append({
                    'id': data['id'],
                    'url': data['url'],
                    'text_sample': text[:200],
                    'issues': issues,
                    'chinese_ratio': chinese_ratio,
                    'traditional_ratio': data['traditional_ratio']
                })
            else:
                good_samples.append({
                    'id': data['id'],
                    'url': data['url'],
                    'text_sample': text[:200],
                    'chinese_ratio': chinese_ratio,
                    'traditional_ratio': data['traditional_ratio']
                })
    
    print(f"\nåˆ†æç»“æœ:")
    print(f"å¥½çš„æ ·æœ¬: {len(good_samples)}")
    print(f"æœ‰é—®é¢˜çš„æ ·æœ¬: {len(bad_samples)}")
    
    print(f"\né—®é¢˜åˆ†å¸ƒ:")
    issue_counts = {}
    for sample in bad_samples:
        for issue in sample['issues']:
            issue_counts[issue] = issue_counts.get(issue, 0) + 1
    for issue, count in issue_counts.items():
        print(f"  {issue}: {count}")
    
    print(f"\nå¥½çš„æ ·æœ¬ç¤ºä¾‹:")
    for i, sample in enumerate(good_samples[:3]):
        print(f"  {i+1}. ID: {sample['id']}")
        print(f"     URL: {sample['url']}")
        print(f"     ç¹ä½“æ¯”ä¾‹: {sample['traditional_ratio']:.3f}")
        print(f"     ä¸­æ–‡æ¯”ä¾‹: {sample['chinese_ratio']:.3f}")
        print(f"     å†…å®¹: {sample['text_sample'][:100]}...")
        print()
    
    return good_samples, bad_samples

def create_clean_dataset():
    """åˆ›å»ºæ¸…ç†åçš„æ•°æ®é›†"""
    good_samples, bad_samples = analyze_existing_data()
    
    print("åˆ›å»ºæ¸…ç†åçš„æ•°æ®é›†...")
    
    # å®šä¹‰è¿‡æ»¤æ¡ä»¶
    def is_clean_traditional_chinese(text, min_ratio=0.02):
        # æ£€æŸ¥æˆäººå†…å®¹
        adult_keywords = ['AV', 'åšçˆ±', 'æ“é€¼', 'è‰²æƒ…', 'æ— ç ', 'æœ‰ç ', 'æˆäºº', 'ä¸‰çº§', 'porn', 'sex']
        if any(keyword.lower() in text.lower() for keyword in adult_keywords):
            return False
        
        # æ£€æŸ¥æ—¥è¯­å¹²æ‰°
        japanese_chars = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', text))
        if japanese_chars > len(text) * 0.05:  # æ—¥è¯­å­—ç¬¦è¶…è¿‡5%
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸»è¦æ˜¯å¯¼èˆªå†…å®¹
        nav_keywords = ['é¦–é¡µ', 'è”ç³»æˆ‘ä»¬', 'å…³äºæˆ‘ä»¬', 'ç‰ˆæƒæ‰€æœ‰', 'äº§å“å¤§å…¨', 'å‹æƒ…é“¾æ¥', 'æ›´å¤š', 'ä¸‹ä¸€é¡µ', 'ä¸Šä¸€é¡µ']
        nav_count = sum(1 for keyword in nav_keywords if keyword in text)
        if nav_count >= 3 and len(text) < 500:
            return False
        
        # æ£€æŸ¥ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        chinese_ratio = chinese_chars / len(text) if text else 0
        if chinese_ratio < 0.3:
            return False
        
        # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯ç¹ä½“ä¸­æ–‡
        simplified = converter_t2s.convert(text)
        diff_simplified = sum(1 for a, b in zip(text, simplified) if a != b)
        traditional_ratio = diff_simplified / len(text) if text else 0
        
        return traditional_ratio >= min_ratio
    
    clean_count = 0
    output_file = 'fineweb-zhtw/data/output_high_quality/cleaned_traditional_chinese.jsonl'
    
    with open('fineweb-zhtw/data/output_improved/traditional_chinese_CC-MAIN-20240612140424-20240612170424-00001.warc.jsonl', 'r', encoding='utf-8') as f:
        with open(output_file, 'w', encoding='utf-8') as out_f:
            for line in f:
                data = json.loads(line)
                text = data['text']
                
                if is_clean_traditional_chinese(text):
                    # é‡æ–°è®¡ç®—ç¹ä½“æ¯”ä¾‹
                    simplified = converter_t2s.convert(text)
                    diff_simplified = sum(1 for a, b in zip(text, simplified) if a != b)
                    traditional_ratio = diff_simplified / len(text) if text else 0
                    
                    # è®¡ç®—ä¸­æ–‡æ¯”ä¾‹
                    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
                    chinese_ratio = chinese_chars / len(text) if text else 0
                    
                    clean_record = {
                        'id': data['id'],
                        'url': data['url'],
                        'text': text,
                        'text_length': len(text),
                        'traditional_ratio': traditional_ratio,
                        'chinese_ratio': chinese_ratio,
                        'is_traditional_chinese': True,
                        'source_file': data['source_file']
                    }
                    
                    out_f.write(json.dumps(clean_record, ensure_ascii=False) + '\n')
                    clean_count += 1
    
    print(f"âœ… æ¸…ç†å®Œæˆï¼Œä¿å­˜äº† {clean_count} æ¡æ¸…æ´çš„ç¹ä½“ä¸­æ–‡è®°å½•")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    return clean_count

if __name__ == "__main__":
    create_clean_dataset()

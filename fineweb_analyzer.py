#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FineWeb Domain Analyzer - å®Œæ•´çš„ç¶²åŸŸåˆ†æå’Œå…§å®¹æ¸…æ´—å·¥å…·

åŠŸèƒ½:
1. WARC è½‰ JSON
2. åŸŸååˆ†æå’Œæå–
3. Robots.txt å¯çˆ¬æ€§æª¢æŸ¥
4. åŸºæ–¼ robots.txt çš„å…§å®¹éæ¿¾

ä½œè€…: FineWeb Domain Analyzer Team
ç‰ˆæœ¬: 1.0.0
è¨±å¯: MIT License
"""

import json
import os
import argparse
import sys
from datetime import datetime
from pathlib import Path
import gzip
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict, Counter
from urllib.parse import urlparse
import logging

# å¯é¸ä¾è³´é …ç›®æª¢æŸ¥
try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False
    print("è­¦å‘Š: requests æœªå®‰è£ï¼Œrobots.txtæª¢æŸ¥åŠŸèƒ½å°‡ä¸å¯ç”¨")

try:
    import warcio
    from warcio.archiveiterator import ArchiveIterator
    HAS_WARCIO = True
except ImportError:
    HAS_WARCIO = False
    print("è­¦å‘Š: warcio æœªå®‰è£ï¼ŒWARCè½‰æ›åŠŸèƒ½å°‡ä¸å¯ç”¨")

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class WARCToJSONConverter:
    """WARC åˆ° JSON è½‰æ›å™¨"""
    
    def __init__(self, output_dir="output", verbose=False):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.verbose = verbose
        
        if not HAS_WARCIO:
            raise ImportError("éœ€è¦å®‰è£ warcio: pip install warcio")
    
    def convert_warc_to_json(self, warc_path, max_records=None):
        """å°‡ WARC æ–‡ä»¶è½‰æ›ç‚º JSON"""
        warc_path = Path(warc_path)
        if not warc_path.exists():
            raise FileNotFoundError(f"WARC æ–‡ä»¶ä¸å­˜åœ¨: {warc_path}")
        
        output_file = self.output_dir / f"{warc_path.stem}.jsonl"
        
        if self.verbose:
            print(f"ğŸ”„ é–‹å§‹è½‰æ›: {warc_path}")
            print(f"ğŸ“ è¼¸å‡ºæ–‡ä»¶: {output_file}")
        
        records_processed = 0
        
        # è™•ç†å£“ç¸®å’Œéå£“ç¸®æ–‡ä»¶
        if warc_path.suffix == '.gz':
            file_obj = gzip.open(warc_path, 'rb')
        else:
            file_obj = open(warc_path, 'rb')
        
        try:
            with open(output_file, 'w', encoding='utf-8') as out_f:
                for record in ArchiveIterator(file_obj):
                    if record.rec_type == 'response':
                        # æå–è¨˜éŒ„ä¿¡æ¯
                        json_record = {
                            'url': record.rec_headers.get_header('WARC-Target-URI'),
                            'timestamp': record.rec_headers.get_header('WARC-Date'),
                            'content_type': record.http_headers.get_header('Content-Type') if record.http_headers else None,
                            'content_length': record.rec_headers.get_header('Content-Length'),
                            'status_code': record.http_headers.get_statuscode() if record.http_headers else None,
                            'content': record.content_stream().read().decode('utf-8', errors='ignore')
                        }
                        
                        out_f.write(json.dumps(json_record, ensure_ascii=False) + '\n')
                        records_processed += 1
                        
                        if max_records and records_processed >= max_records:
                            break
                        
                        if self.verbose and records_processed % 1000 == 0:
                            print(f"  å·²è™•ç† {records_processed} æ¢è¨˜éŒ„")
        
        finally:
            file_obj.close()
        
        if self.verbose:
            print(f"âœ… è½‰æ›å®Œæˆï¼å…±è™•ç† {records_processed} æ¢è¨˜éŒ„")
        
        return output_file, records_processed

class DomainExtractor:
    """åŸŸåæå–å’Œåˆ†æå™¨"""
    
    def __init__(self, verbose=False):
        self.verbose = verbose
        self.domain_stats = defaultdict(lambda: {
            'count': 0,
            'urls': [],
            'tld': '',
            'first_seen': None,
            'last_seen': None
        })
    
    def extract_domain(self, url):
        """å¾URLæå–åŸŸå"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # ç§»é™¤ç«¯å£è™Ÿ
            if ':' in domain and not domain.endswith(':8080'):
                domain = domain.split(':')[0]
            
            # å¯é¸: ç§»é™¤wwwå‰ç¶´
            if domain.startswith('www.'):
                domain = domain[4:]
            
            return domain
        except Exception as e:
            if self.verbose:
                logger.warning(f"URLè§£æéŒ¯èª¤ {url}: {e}")
            return None
    
    def extract_tld(self, domain):
        """æå–é ‚ç´šåŸŸå"""
        if not domain or '.' not in domain:
            return ''
        return domain.split('.')[-1]
    
    def analyze_jsonl_file(self, jsonl_path):
        """åˆ†æJSONLæ–‡ä»¶ä¸­çš„åŸŸå"""
        jsonl_path = Path(jsonl_path)
        if not jsonl_path.exists():
            raise FileNotFoundError(f"JSONL æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_path}")
        
        if self.verbose:
            print(f"ğŸ” åˆ†ææ–‡ä»¶: {jsonl_path}")
        
        processed_count = 0
        
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if line.strip():
                    try:
                        record = json.loads(line)
                        url = record.get('url', '')
                        
                        if url:
                            domain = self.extract_domain(url)
                            if domain:
                                stats = self.domain_stats[domain]
                                stats['count'] += 1
                                stats['tld'] = self.extract_tld(domain)
                                
                                # è¨˜éŒ„URLç¤ºä¾‹ï¼ˆæœ€å¤š5å€‹ï¼‰
                                if len(stats['urls']) < 5:
                                    stats['urls'].append(url)
                                
                                # è¨˜éŒ„æ™‚é–“æˆ³
                                timestamp = record.get('timestamp') or datetime.now().isoformat()
                                if stats['first_seen'] is None:
                                    stats['first_seen'] = timestamp
                                stats['last_seen'] = timestamp
                                
                                processed_count += 1
                                
                                if self.verbose and processed_count % 1000 == 0:
                                    print(f"  å·²è™•ç† {processed_count} å€‹URL")
                    
                    except json.JSONDecodeError as e:
                        if self.verbose:
                            logger.warning(f"ç¬¬{line_num}è¡ŒJSONè§£æéŒ¯èª¤: {e}")
        
        if self.verbose:
            print(f"âœ… åˆ†æå®Œæˆï¼ç™¼ç¾ {len(self.domain_stats)} å€‹å”¯ä¸€åŸŸå")
        
        return dict(self.domain_stats)
    
    def save_domain_analysis(self, output_dir="output"):
        """ä¿å­˜åŸŸååˆ†æçµæœ"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ç°¡å–®åŸŸååˆ—è¡¨
        domains_list = sorted(self.domain_stats.keys())
        simple_output = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_domains": len(domains_list),
                "format": "simple_list"
            },
            "domains": domains_list
        }
        
        simple_file = output_dir / f"domains_simple_{timestamp}.json"
        with open(simple_file, 'w', encoding='utf-8') as f:
            json.dump(simple_output, f, ensure_ascii=False, indent=2)
        
        # è©³ç´°çµ±è¨ˆ
        detailed_output = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_domains": len(self.domain_stats),
                "format": "detailed_stats"
            },
            "domains": dict(self.domain_stats)
        }
        
        detailed_file = output_dir / f"domains_detailed_{timestamp}.json"
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_output, f, ensure_ascii=False, indent=2)
        
        if self.verbose:
            print(f"ğŸ’¾ åŸŸååˆ†æå·²ä¿å­˜:")
            print(f"  ç°¡å–®åˆ—è¡¨: {simple_file}")
            print(f"  è©³ç´°çµ±è¨ˆ: {detailed_file}")
        
        return simple_file, detailed_file

class RobotsChecker:
    """Robots.txt æª¢æŸ¥å™¨"""
    
    def __init__(self, user_agent="*", timeout=10, max_workers=10, verbose=False):
        if not HAS_REQUESTS:
            raise ImportError("éœ€è¦å®‰è£ requests: pip install requests")
        
        self.user_agent = user_agent
        self.timeout = timeout
        self.max_workers = max_workers
        self.verbose = verbose
        self.results = {}
        
        # è¨­ç½®æœƒè©±
        self.session = requests.Session()
        
        # è¨­ç½®é‡è©¦ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # è¨­ç½®è«‹æ±‚é ­
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; FineWebAnalyzer/1.0)',
            'Accept': 'text/plain,text/html,*/*',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        })
    
    def check_robots_txt(self, domain):
        """æª¢æŸ¥å–®å€‹åŸŸåçš„robots.txt"""
        result = {
            'domain': domain,
            'robots_exists': False,
            'crawl_allowed': True,
            'crawl_delay': None,
            'disallowed_paths': [],
            'error': None,
            'last_checked': datetime.now().isoformat()
        }
        
        try:
            for protocol in ['https', 'http']:
                robots_url = f"{protocol}://{domain}/robots.txt"
                
                try:
                    response = self.session.get(robots_url, timeout=self.timeout)
                    
                    if response.status_code == 200:
                        result['robots_exists'] = True
                        result['robots_content'] = response.text
                        self._parse_robots_content(result, response.text)
                        break
                    elif response.status_code == 404:
                        result['crawl_allowed'] = True
                        break
                
                except requests.exceptions.RequestException as e:
                    if protocol == 'http':  # æœ€å¾Œä¸€æ¬¡å˜—è©¦å¤±æ•—
                        result['error'] = f"é€£æ¥å¤±æ•—: {str(e)[:100]}"
                        result['crawl_allowed'] = False
                    continue
        
        except Exception as e:
            result['error'] = f"æª¢æŸ¥éŒ¯èª¤: {str(e)}"
            result['crawl_allowed'] = False
        
        return result
    
    def _parse_robots_content(self, result, robots_content):
        """è§£ærobots.txtå…§å®¹"""
        lines = robots_content.strip().split('\n')
        current_user_agent = None
        applies_to_us = False
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            
            if line.lower().startswith('user-agent:'):
                current_user_agent = line.split(':', 1)[1].strip()
                applies_to_us = (current_user_agent == '*' or 
                               current_user_agent.lower() == self.user_agent.lower())
            
            elif applies_to_us:
                if line.lower().startswith('disallow:'):
                    path = line.split(':', 1)[1].strip()
                    if path == '/':
                        result['crawl_allowed'] = False
                    elif path:
                        result['disallowed_paths'].append(path)
                
                elif line.lower().startswith('crawl-delay:'):
                    try:
                        delay = float(line.split(':', 1)[1].strip())
                        result['crawl_delay'] = delay
                    except ValueError:
                        pass
    
    def check_domains_batch(self, domains, output_dir="output"):
        """æ‰¹é‡æª¢æŸ¥åŸŸå"""
        if not domains:
            return {}
        
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        if self.verbose:
            print(f"ğŸ¤– é–‹å§‹æª¢æŸ¥ {len(domains)} å€‹åŸŸåçš„robots.txt...")
        
        # ä¸¦ç™¼æª¢æŸ¥
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_domain = {
                executor.submit(self.check_robots_txt, domain): domain 
                for domain in domains
            }
            
            for future in as_completed(future_to_domain):
                domain = future_to_domain[future]
                try:
                    result = future.result(timeout=self.timeout + 5)
                    self.results[domain] = result
                    
                    if self.verbose:
                        status = "âœ…" if result['crawl_allowed'] else "âŒ"
                        print(f"  {status} {domain}: {'å¯çˆ¬' if result['crawl_allowed'] else 'ç¦æ­¢'}")
                
                except Exception as e:
                    self.results[domain] = {
                        'domain': domain,
                        'error': str(e),
                        'crawl_allowed': False,
                        'last_checked': datetime.now().isoformat()
                    }
                    if self.verbose:
                        print(f"  âŒ {domain}: {str(e)[:50]}")
        
        # ä¿å­˜çµæœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = output_dir / f"robots_check_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'total_domains': len(domains),
                    'user_agent': self.user_agent,
                    'timeout': self.timeout
                },
                'results': self.results
            }, f, ensure_ascii=False, indent=2)
        
        if self.verbose:
            crawlable_count = sum(1 for r in self.results.values() if r.get('crawl_allowed', False))
            print(f"âœ… æª¢æŸ¥å®Œæˆï¼{crawlable_count}/{len(domains)} å€‹åŸŸåå¯çˆ¬å–")
            print(f"ğŸ’¾ çµæœå·²ä¿å­˜: {results_file}")
        
        return self.results, results_file

class ContentFilter:
    """åŸºæ–¼robots.txtçš„å…§å®¹éæ¿¾å™¨"""
    
    def __init__(self, verbose=False):
        self.verbose = verbose
    
    def filter_jsonl_by_robots(self, jsonl_path, robots_results, output_dir="output"):
        """æ ¹æ“šrobots.txtçµæœéæ¿¾JSONLå…§å®¹"""
        jsonl_path = Path(jsonl_path)
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        if not jsonl_path.exists():
            raise FileNotFoundError(f"JSONL æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_path}")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filtered_file = output_dir / f"filtered_{jsonl_path.stem}_{timestamp}.jsonl"
        excluded_file = output_dir / f"excluded_{jsonl_path.stem}_{timestamp}.jsonl"
        
        if self.verbose:
            print(f"ğŸ”„ é–‹å§‹éæ¿¾: {jsonl_path}")
            print(f"ğŸ“ éæ¿¾å¾Œæ–‡ä»¶: {filtered_file}")
            print(f"ğŸ“ æ’é™¤å…§å®¹æ–‡ä»¶: {excluded_file}")
        
        # å»ºç«‹åŸŸååˆ°å¯çˆ¬æ€§çš„æ˜ å°„
        domain_crawlable = {}
        for domain, result in robots_results.items():
            domain_crawlable[domain] = result.get('crawl_allowed', False)
        
        processed_count = 0
        filtered_count = 0
        excluded_count = 0
        
        with open(jsonl_path, 'r', encoding='utf-8') as input_f, \
             open(filtered_file, 'w', encoding='utf-8') as filtered_f, \
             open(excluded_file, 'w', encoding='utf-8') as excluded_f:
            
            for line_num, line in enumerate(input_f, 1):
                if line.strip():
                    try:
                        record = json.loads(line)
                        url = record.get('url', '')
                        
                        if url:
                            # æå–åŸŸå
                            domain = self._extract_domain(url)
                            
                            # æª¢æŸ¥æ˜¯å¦å¯çˆ¬
                            if domain and domain_crawlable.get(domain, True):  # é»˜èªå…è¨±
                                filtered_f.write(line)
                                filtered_count += 1
                            else:
                                excluded_f.write(line)
                                excluded_count += 1
                        else:
                            # æ²’æœ‰URLçš„è¨˜éŒ„ä¿ç•™
                            filtered_f.write(line)
                            filtered_count += 1
                        
                        processed_count += 1
                        
                        if self.verbose and processed_count % 1000 == 0:
                            print(f"  å·²è™•ç† {processed_count} æ¢è¨˜éŒ„")
                    
                    except json.JSONDecodeError as e:
                        if self.verbose:
                            logger.warning(f"ç¬¬{line_num}è¡ŒJSONè§£æéŒ¯èª¤: {e}")
        
        # ç”Ÿæˆçµ±è¨ˆå ±å‘Š
        stats = {
            'total_processed': processed_count,
            'filtered_kept': filtered_count,
            'excluded_count': excluded_count,
            'keep_ratio': round(filtered_count / processed_count * 100, 2) if processed_count > 0 else 0
        }
        
        stats_file = output_dir / f"filter_stats_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'source_file': str(jsonl_path),
                    'filtered_file': str(filtered_file),
                    'excluded_file': str(excluded_file)
                },
                'statistics': stats
            }, f, ensure_ascii=False, indent=2)
        
        if self.verbose:
            print(f"âœ… éæ¿¾å®Œæˆï¼")
            print(f"  ğŸ“Š ä¿ç•™: {filtered_count} æ¢ ({stats['keep_ratio']}%)")
            print(f"  ğŸ—‘ï¸ æ’é™¤: {excluded_count} æ¢")
            print(f"  ğŸ“ˆ çµ±è¨ˆ: {stats_file}")
        
        return filtered_file, excluded_file, stats
    
    def _extract_domain(self, url):
        """å¾URLæå–åŸŸå"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            if ':' in domain and not domain.endswith(':8080'):
                domain = domain.split(':')[0]
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except:
            return None

def main():
    parser = argparse.ArgumentParser(
        description="FineWeb Domain Analyzer - å®Œæ•´çš„ç¶²åŸŸåˆ†æå’Œå…§å®¹æ¸…æ´—å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å®Œæ•´æµç¨‹
  python fineweb_analyzer.py --input data.warc.gz --all-steps

  # å–®æ­¥æ“ä½œ
  python fineweb_analyzer.py --input data.warc.gz --warc-to-json
  python fineweb_analyzer.py --input data.jsonl --extract-domains
  python fineweb_analyzer.py --domains domains.json --check-robots
  python fineweb_analyzer.py --input data.jsonl --robots robots_check.json --filter-content
        """
    )
    
    # åŸºæœ¬åƒæ•¸
    parser.add_argument('--input', '-i', help='è¼¸å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', default='output', help='è¼¸å‡ºç›®éŒ„ (é»˜èª: output)')
    parser.add_argument('--verbose', '-v', action='store_true', help='è©³ç´°è¼¸å‡º')
    
    # åŠŸèƒ½é¸æ“‡
    parser.add_argument('--all-steps', action='store_true', help='åŸ·è¡Œå®Œæ•´æµç¨‹ (1-4æ­¥)')
    parser.add_argument('--warc-to-json', action='store_true', help='æ­¥é©Ÿ1: WARCè½‰JSON')
    parser.add_argument('--extract-domains', action='store_true', help='æ­¥é©Ÿ2: æå–åŸŸå')
    parser.add_argument('--check-robots', action='store_true', help='æ­¥é©Ÿ3: æª¢æŸ¥robots.txt')
    parser.add_argument('--filter-content', action='store_true', help='æ­¥é©Ÿ4: éæ¿¾å…§å®¹')
    
    # é¡å¤–åƒæ•¸
    parser.add_argument('--domains', help='åŸŸåæ–‡ä»¶è·¯å¾„ (ç”¨æ–¼æ­¥é©Ÿ3)')
    parser.add_argument('--robots', help='robotsæª¢æŸ¥çµæœæ–‡ä»¶ (ç”¨æ–¼æ­¥é©Ÿ4)')
    parser.add_argument('--max-records', type=int, help='æœ€å¤§è™•ç†è¨˜éŒ„æ•¸')
    parser.add_argument('--timeout', type=int, default=10, help='è«‹æ±‚è¶…æ™‚æ™‚é–“ (é»˜èª: 10ç§’)')
    parser.add_argument('--max-workers', type=int, default=10, help='æœ€å¤§ä½µç™¼æ•¸ (é»˜èª: 10)')
    
    args = parser.parse_args()
    
    # åƒæ•¸é©—è­‰
    if args.all_steps or args.warc_to_json or args.extract_domains or args.filter_content:
        if not args.input:
            parser.error("é€™äº›æ“ä½œéœ€è¦ --input åƒæ•¸")
    
    if args.check_robots and not args.domains:
        parser.error("--check-robots éœ€è¦ --domains åƒæ•¸")
    
    if args.filter_content and not args.robots:
        parser.error("--filter-content éœ€è¦ --robots åƒæ•¸")
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = Path(args.output)
    output_dir.mkdir(exist_ok=True)
    
    try:
        if args.all_steps:
            print("ğŸš€ é–‹å§‹åŸ·è¡Œå®Œæ•´æµç¨‹...")
            
            # æ­¥é©Ÿ1: WARCè½‰JSON
            if Path(args.input).suffix in ['.warc', '.gz']:
                print("\nğŸ“‹ æ­¥é©Ÿ1: WARCè½‰JSON")
                converter = WARCToJSONConverter(args.output, args.verbose)
                jsonl_file, _ = converter.convert_warc_to_json(args.input, args.max_records)
            else:
                jsonl_file = Path(args.input)
            
            # æ­¥é©Ÿ2: æå–åŸŸå
            print("\nğŸ“‹ æ­¥é©Ÿ2: åŸŸååˆ†æ")
            extractor = DomainExtractor(args.verbose)
            domain_stats = extractor.analyze_jsonl_file(jsonl_file)
            simple_file, _ = extractor.save_domain_analysis(args.output)
            
            # æ­¥é©Ÿ3: æª¢æŸ¥robots.txt
            print("\nğŸ“‹ æ­¥é©Ÿ3: Robots.txtæª¢æŸ¥")
            checker = RobotsChecker(timeout=args.timeout, max_workers=args.max_workers, verbose=args.verbose)
            domains = list(domain_stats.keys())
            robots_results, robots_file = checker.check_domains_batch(domains, args.output)
            
            # æ­¥é©Ÿ4: éæ¿¾å…§å®¹
            print("\nğŸ“‹ æ­¥é©Ÿ4: å…§å®¹éæ¿¾")
            filter_tool = ContentFilter(args.verbose)
            filtered_file, excluded_file, stats = filter_tool.filter_jsonl_by_robots(
                jsonl_file, robots_results, args.output
            )
            
            print(f"\nğŸ‰ å®Œæ•´æµç¨‹åŸ·è¡Œå®Œæˆï¼")
            print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {args.output}")
            
        else:
            # å–®æ­¥åŸ·è¡Œ
            if args.warc_to_json:
                converter = WARCToJSONConverter(args.output, args.verbose)
                converter.convert_warc_to_json(args.input, args.max_records)
            
            elif args.extract_domains:
                extractor = DomainExtractor(args.verbose)
                extractor.analyze_jsonl_file(args.input)
                extractor.save_domain_analysis(args.output)
            
            elif args.check_robots:
                with open(args.domains, 'r', encoding='utf-8') as f:
                    domain_data = json.load(f)
                    domains = domain_data.get('domains', [])
                
                checker = RobotsChecker(timeout=args.timeout, max_workers=args.max_workers, verbose=args.verbose)
                checker.check_domains_batch(domains, args.output)
            
            elif args.filter_content:
                with open(args.robots, 'r', encoding='utf-8') as f:
                    robots_data = json.load(f)
                    robots_results = robots_data.get('results', {})
                
                filter_tool = ContentFilter(args.verbose)
                filter_tool.filter_jsonl_by_robots(args.input, robots_results, args.output)
            
            else:
                parser.print_help()
                sys.exit(1)
    
    except Exception as e:
        logger.error(f"åŸ·è¡ŒéŒ¯èª¤: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()

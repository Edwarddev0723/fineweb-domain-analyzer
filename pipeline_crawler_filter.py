#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„çˆ¬å–æ¬Šé™éæ¿¾æµç¨‹
æ­¥é©Ÿ1: å¾è³‡æ–™é›†æå–åŸŸå
æ­¥é©Ÿ2: æª¢æŸ¥åŸŸåçš„robots.txtå’Œçˆ¬å–æ¬Šé™
æ­¥é©Ÿ3: æ ¹æ“šçˆ¬å–æ¬Šé™éæ¿¾åŸå§‹æ•¸æ“š

ä½œè€…: AIåŠ©æ‰‹
æ—¥æœŸ: 2025-07-18
"""

import json
import os
import sys
import time
import argparse
from datetime import datetime
from urllib.parse import urlparse
from collections import defaultdict, Counter

# å°å…¥æˆ‘å€‘çš„æ¨¡å¡Š
from extract_domains import DomainExtractor, JSONOutputGenerator
from check_crawlability import RobotsChecker, CrawlabilityAnalyzer

def load_jsonl_data(file_path):
    """å¾JSONLæ–‡ä»¶åŠ è¼‰æ•¸æ“š"""
    data = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if line.strip():
                    try:
                        data.append(json.loads(line))
                    except json.JSONDecodeError as e:
                        print(f"ç¬¬{line_num}è¡ŒJSONè§£æéŒ¯èª¤: {e}")
        return data
    except FileNotFoundError:
        print(f"æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return []
    except Exception as e:
        print(f"è®€å–æ–‡ä»¶æ™‚å‡ºéŒ¯: {e}")
        return []

class CrawlerFilterPipeline:
    """å®Œæ•´çš„çˆ¬å–æ¬Šé™éæ¿¾æµç¨‹"""
    
    def __init__(self, verbose=True, timeout=10, max_workers=10):
        self.verbose = verbose
        self.timeout = timeout
        self.max_workers = max_workers
        self.pipeline_results = {
            'step1_domains': {},
            'step2_crawlability': {},
            'step3_filtered': {},
            'pipeline_stats': {}
        }
    
    def log(self, message):
        """è¼¸å‡ºæ—¥èªŒ"""
        if self.verbose:
            timestamp = datetime.now().strftime("%H:%M:%S")
            print(f"[{timestamp}] {message}")
    
    def step1_extract_domains(self, input_files, output_dir="pipeline_output"):
        """
        æ­¥é©Ÿ1: å¾è³‡æ–™é›†æå–åŸŸå
        """
        self.log("ğŸš€ æ­¥é©Ÿ1: æå–åŸŸåé–‹å§‹")
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è¼‰æ‰€æœ‰æ•¸æ“šæ–‡ä»¶
        all_data = []
        for file_path in input_files:
            if os.path.exists(file_path):
                self.log(f"ğŸ“‚ åŠ è¼‰æ–‡ä»¶: {file_path}")
                file_data = load_jsonl_data(file_path)
                self.log(f"  â””â”€ æˆåŠŸåŠ è¼‰ {len(file_data)} æ¢è¨˜éŒ„")
                all_data.extend(file_data)
            else:
                self.log(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        if not all_data:
            raise ValueError("æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•¸æ“šæ–‡ä»¶")
        
        self.log(f"âœ… ç¸½å…±åŠ è¼‰äº† {len(all_data)} æ¢è¨˜éŒ„")
        
        # æå–åŸŸå
        extractor = DomainExtractor()
        domain_data = extractor.process_urls(all_data)
        
        # ç”ŸæˆåŸŸååˆ—è¡¨
        generator = JSONOutputGenerator(domain_data)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ç°¡å–®åŸŸååˆ—è¡¨ç”¨æ–¼ä¸‹ä¸€æ­¥
        simple_domains = list(domain_data.keys())
        domain_file = os.path.join(output_dir, f"extracted_domains_{timestamp}.json")
        
        with open(domain_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'total_domains': len(simple_domains),
                    'total_urls_processed': len(all_data),
                    'format': 'simple_list',
                    'pipeline_step': 1
                },
                'domains': simple_domains
            }, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è©³ç´°çµ±è¨ˆ
        detailed_file = os.path.join(output_dir, f"domain_stats_{timestamp}.json")
        detailed_data = generator.generate_detailed_stats()
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_data, f, ensure_ascii=False, indent=2)
        
        # æ›´æ–°pipelineçµæœ
        self.pipeline_results['step1_domains'] = {
            'domain_file': domain_file,
            'detailed_file': detailed_file,
            'total_domains': len(simple_domains),
            'total_urls': len(all_data),
            'domain_list': simple_domains
        }
        
        self.log(f"âœ… æ­¥é©Ÿ1å®Œæˆ: æå–å‡º {len(simple_domains)} å€‹å”¯ä¸€åŸŸå")
        self.log(f"ğŸ“ åŸŸåæ–‡ä»¶: {domain_file}")
        
        return domain_file, simple_domains
    
    def step2_check_crawlability(self, domain_file, output_dir="pipeline_output"):
        """
        æ­¥é©Ÿ2: æª¢æŸ¥åŸŸåçš„robots.txtå’Œçˆ¬å–æ¬Šé™
        """
        self.log("ğŸ¤– æ­¥é©Ÿ2: æª¢æŸ¥çˆ¬å–æ¬Šé™é–‹å§‹")
        
        # è®€å–åŸŸåæ–‡ä»¶
        with open(domain_file, 'r', encoding='utf-8') as f:
            domain_data = json.load(f)
        
        domains = domain_data.get('domains', [])
        self.log(f"ğŸŒ æº–å‚™æª¢æŸ¥ {len(domains)} å€‹åŸŸå")
        
        # åˆå§‹åŒ–æª¢æŸ¥å™¨
        checker = RobotsChecker(
            user_agent="*",
            timeout=self.timeout,
            max_workers=self.max_workers
        )
        
        # åŸ·è¡Œæ‰¹é‡æª¢æŸ¥
        results = checker.check_domains_batch(domains, verbose=self.verbose)
        
        # åˆ†æçµæœ
        analyzer = CrawlabilityAnalyzer(results)
        stats = analyzer.analyze_crawlability()
        recommendations = analyzer.generate_recommendations(stats)
        
        # ç”ŸæˆåŸŸåæ¨™ç±¤ (1=å¯çˆ¬, 0=ä¸å¯çˆ¬)
        domain_labels = {}
        crawlable_domains = set()
        non_crawlable_domains = set()
        
        for domain, result in results.items():
            if result.get('crawl_allowed', False) and not result.get('error'):
                domain_labels[domain] = 1
                crawlable_domains.add(domain)
            else:
                domain_labels[domain] = 0
                non_crawlable_domains.add(domain)
        
        # ä¿å­˜çµæœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. è©³ç´°æª¢æŸ¥çµæœ
        results_file = os.path.join(output_dir, f"crawlability_results_{timestamp}.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'total_domains': len(domains),
                    'pipeline_step': 2,
                    'user_agent': "*",
                    'timeout': self.timeout
                },
                'results': results,
                'statistics': stats,
                'recommendations': recommendations
            }, f, ensure_ascii=False, indent=2)
        
        # 2. åŸŸåæ¨™ç±¤æ–‡ä»¶ (é—œéµè¼¸å‡º)
        labels_file = os.path.join(output_dir, f"domain_labels_{timestamp}.json")
        with open(labels_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'total_domains': len(domain_labels),
                    'crawlable_count': len(crawlable_domains),
                    'non_crawlable_count': len(non_crawlable_domains),
                    'crawlable_rate': len(crawlable_domains) / len(domain_labels) * 100,
                    'pipeline_step': 2,
                    'format': 'domain_labels'
                },
                'domain_labels': domain_labels,
                'crawlable_domains': list(crawlable_domains),
                'non_crawlable_domains': list(non_crawlable_domains)
            }, f, ensure_ascii=False, indent=2)
        
        # æ›´æ–°pipelineçµæœ
        self.pipeline_results['step2_crawlability'] = {
            'results_file': results_file,
            'labels_file': labels_file,
            'total_checked': len(domains),
            'crawlable_count': len(crawlable_domains),
            'non_crawlable_count': len(non_crawlable_domains),
            'crawlable_rate': len(crawlable_domains) / len(domain_labels) * 100,
            'domain_labels': domain_labels
        }
        
        self.log(f"âœ… æ­¥é©Ÿ2å®Œæˆ: {len(crawlable_domains)} å€‹å¯çˆ¬å–, {len(non_crawlable_domains)} å€‹ä¸å¯çˆ¬å–")
        self.log(f"ğŸ“Š å¯çˆ¬å–ç‡: {len(crawlable_domains) / len(domain_labels) * 100:.1f}%")
        self.log(f"ğŸ“ æ¨™ç±¤æ–‡ä»¶: {labels_file}")
        
        return labels_file, domain_labels
    
    def step3_filter_data(self, input_files, domain_labels, output_dir="pipeline_output"):
        """
        æ­¥é©Ÿ3: æ ¹æ“šåŸŸåæ¨™ç±¤éæ¿¾åŸå§‹æ•¸æ“š
        """
        self.log("ğŸ” æ­¥é©Ÿ3: éæ¿¾åŸå§‹æ•¸æ“šé–‹å§‹")
        
        # çµ±è¨ˆä¿¡æ¯
        total_records = 0
        filtered_records = 0
        crawlable_records = 0
        non_crawlable_records = 0
        unknown_domain_records = 0
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ç‚ºæ¯å€‹è¼¸å…¥æ–‡ä»¶å‰µå»ºéæ¿¾å¾Œçš„ç‰ˆæœ¬
        filtered_files = []
        
        for input_file in input_files:
            if not os.path.exists(input_file):
                self.log(f"âš ï¸ è·³éä¸å­˜åœ¨çš„æ–‡ä»¶: {input_file}")
                continue
            
            self.log(f"ğŸ“‚ è™•ç†æ–‡ä»¶: {input_file}")
            
            # ç”Ÿæˆè¼¸å‡ºæ–‡ä»¶å
            base_name = os.path.splitext(os.path.basename(input_file))[0]
            output_file = os.path.join(output_dir, f"filtered_{base_name}_{timestamp}.jsonl")
            rejected_file = os.path.join(output_dir, f"rejected_{base_name}_{timestamp}.jsonl")
            
            file_total = 0
            file_kept = 0
            file_rejected = 0
            file_unknown = 0
            
            with open(input_file, 'r', encoding='utf-8') as infile, \
                 open(output_file, 'w', encoding='utf-8') as outfile, \
                 open(rejected_file, 'w', encoding='utf-8') as rejfile:
                
                for line_num, line in enumerate(infile, 1):
                    if not line.strip():
                        continue
                    
                    try:
                        record = json.loads(line)
                        url = record.get('url', '')
                        
                        if not url:
                            # æ²’æœ‰URLçš„è¨˜éŒ„ç›´æ¥æ‹’çµ¶
                            rejfile.write(line)
                            file_rejected += 1
                            continue
                        
                        # æå–åŸŸå
                        try:
                            parsed = urlparse(url)
                            domain = parsed.netloc.lower()
                            if ':' in domain:
                                domain = domain.split(':')[0]
                            if domain.startswith('www.'):
                                domain = domain[4:]
                        except:
                            # URLè§£æå¤±æ•—
                            rejfile.write(line)
                            file_rejected += 1
                            continue
                        
                        # æª¢æŸ¥åŸŸåæ¨™ç±¤
                        if domain in domain_labels:
                            if domain_labels[domain] == 1:  # å¯çˆ¬å–
                                outfile.write(line)
                                file_kept += 1
                                crawlable_records += 1
                            else:  # ä¸å¯çˆ¬å–
                                rejfile.write(line)
                                file_rejected += 1
                                non_crawlable_records += 1
                        else:
                            # æœªçŸ¥åŸŸå - ä¿å®ˆè™•ç†ï¼Œæ‹’çµ¶
                            rejfile.write(line)
                            file_rejected += 1
                            file_unknown += 1
                            unknown_domain_records += 1
                        
                        file_total += 1
                        total_records += 1
                        
                        # é€²åº¦é¡¯ç¤º
                        if line_num % 1000 == 0:
                            self.log(f"  è™•ç†é€²åº¦: {line_num} è¡Œ")
                    
                    except json.JSONDecodeError as e:
                        self.log(f"  âš ï¸ ç¬¬{line_num}è¡ŒJSONè§£æéŒ¯èª¤: {e}")
                        rejfile.write(line)
                        file_rejected += 1
                        continue
            
            filtered_records += file_kept
            
            self.log(f"  âœ… {os.path.basename(input_file)}: {file_kept}/{file_total} è¨˜éŒ„ä¿ç•™ ({file_kept/file_total*100:.1f}%)")
            self.log(f"     ä¿ç•™: {output_file}")
            self.log(f"     æ‹’çµ•: {rejected_file}")
            
            filtered_files.append({
                'input_file': input_file,
                'output_file': output_file,
                'rejected_file': rejected_file,
                'total_records': file_total,
                'kept_records': file_kept,
                'rejected_records': file_rejected,
                'unknown_domains': file_unknown
            })
        
        # ç”Ÿæˆéæ¿¾å ±å‘Š
        report_file = os.path.join(output_dir, f"filtering_report_{timestamp}.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'pipeline_step': 3,
                    'total_input_files': len(input_files),
                    'processed_files': len(filtered_files)
                },
                'summary': {
                    'total_records_processed': total_records,
                    'records_kept': filtered_records,
                    'records_rejected': total_records - filtered_records,
                    'retention_rate': filtered_records / total_records * 100 if total_records > 0 else 0,
                    'crawlable_records': crawlable_records,
                    'non_crawlable_records': non_crawlable_records,
                    'unknown_domain_records': unknown_domain_records
                },
                'file_details': filtered_files
            }, f, ensure_ascii=False, indent=2)
        
        # æ›´æ–°pipelineçµæœ
        self.pipeline_results['step3_filtered'] = {
            'report_file': report_file,
            'filtered_files': filtered_files,
            'total_processed': total_records,
            'total_kept': filtered_records,
            'retention_rate': filtered_records / total_records * 100 if total_records > 0 else 0
        }
        
        self.log(f"âœ… æ­¥é©Ÿ3å®Œæˆ: {filtered_records}/{total_records} è¨˜éŒ„ä¿ç•™ ({filtered_records/total_records*100:.1f}%)")
        self.log(f"ğŸ“ éæ¿¾å ±å‘Š: {report_file}")
        
        return filtered_files, report_file
    
    def run_complete_pipeline(self, input_files, output_dir="pipeline_output"):
        """
        é‹è¡Œå®Œæ•´çš„ä¸‰æ­¥é©Ÿæµç¨‹
        """
        start_time = time.time()
        self.log("ğŸš€ é–‹å§‹å®Œæ•´çš„çˆ¬å–æ¬Šé™éæ¿¾æµç¨‹")
        
        try:
            # æ­¥é©Ÿ1: æå–åŸŸå
            domain_file, domains = self.step1_extract_domains(input_files, output_dir)
            
            # æ­¥é©Ÿ2: æª¢æŸ¥çˆ¬å–æ¬Šé™
            labels_file, domain_labels = self.step2_check_crawlability(domain_file, output_dir)
            
            # æ­¥é©Ÿ3: éæ¿¾æ•¸æ“š
            filtered_files, report_file = self.step3_filter_data(input_files, domain_labels, output_dir)
            
            # ç”Ÿæˆæœ€çµ‚æ‘˜è¦
            elapsed_time = time.time() - start_time
            
            self.pipeline_results['pipeline_stats'] = {
                'total_runtime': elapsed_time,
                'completed_at': datetime.now().isoformat(),
                'success': True
            }
            
            # ä¿å­˜å®Œæ•´çš„pipelineçµæœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            summary_file = os.path.join(output_dir, f"pipeline_summary_{timestamp}.json")
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(self.pipeline_results, f, ensure_ascii=False, indent=2)
            
            self.log("=" * 60)
            self.log("ğŸ‰ å®Œæ•´æµç¨‹åŸ·è¡ŒæˆåŠŸ!")
            self.log(f"â±ï¸  ç¸½è€—æ™‚: {elapsed_time:.1f} ç§’")
            self.log(f"ğŸŒ è™•ç†åŸŸå: {self.pipeline_results['step1_domains']['total_domains']}")
            self.log(f"âœ… å¯çˆ¬å–åŸŸå: {self.pipeline_results['step2_crawlability']['crawlable_count']}")
            self.log(f"ğŸ“Š å¯çˆ¬å–ç‡: {self.pipeline_results['step2_crawlability']['crawlable_rate']:.1f}%")
            self.log(f"ğŸ“ æ•¸æ“šä¿ç•™ç‡: {self.pipeline_results['step3_filtered']['retention_rate']:.1f}%")
            self.log(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
            self.log(f"ğŸ“‹ å®Œæ•´æ‘˜è¦: {summary_file}")
            self.log("=" * 60)
            
            return True, summary_file
            
        except Exception as e:
            self.log(f"âŒ æµç¨‹åŸ·è¡Œå¤±æ•—: {e}")
            self.pipeline_results['pipeline_stats'] = {
                'total_runtime': time.time() - start_time,
                'completed_at': datetime.now().isoformat(),
                'success': False,
                'error': str(e)
            }
            return False, str(e)

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='å®Œæ•´çš„çˆ¬å–æ¬Šé™éæ¿¾æµç¨‹')
    parser.add_argument('input_files', nargs='+', help='è¼¸å…¥çš„JSONLæ•¸æ“šæ–‡ä»¶')
    parser.add_argument('-o', '--output', default='pipeline_output', help='è¼¸å‡ºç›®éŒ„ (é»˜èª: pipeline_output)')
    parser.add_argument('-t', '--timeout', type=int, default=10, help='è«‹æ±‚è¶…æ™‚æ™‚é–“ (é»˜èª: 10ç§’)')
    parser.add_argument('-w', '--workers', type=int, default=10, help='ä¸¦ç™¼æ•¸é‡ (é»˜èª: 10)')
    parser.add_argument('-v', '--verbose', action='store_true', help='é¡¯ç¤ºè©³ç´°è¼¸å‡º')
    parser.add_argument('--step', type=int, choices=[1, 2, 3], help='åªåŸ·è¡Œç‰¹å®šæ­¥é©Ÿ (é»˜èª: åŸ·è¡Œå…¨éƒ¨)')
    
    args = parser.parse_args()
    
    # æª¢æŸ¥è¼¸å…¥æ–‡ä»¶
    valid_files = []
    for file_path in args.input_files:
        if os.path.exists(file_path):
            valid_files.append(file_path)
        else:
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    if not valid_files:
        print("âŒ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¼¸å…¥æ–‡ä»¶")
        sys.exit(1)
    
    # åˆå§‹åŒ–pipeline
    pipeline = CrawlerFilterPipeline(
        verbose=args.verbose,
        timeout=args.timeout,
        max_workers=args.workers
    )
    
    try:
        if args.step:
            print(f"ğŸ¯ åŸ·è¡Œå–®ä¸€æ­¥é©Ÿ: {args.step}")
            # é€™è£¡å¯ä»¥æ·»åŠ å–®æ­¥é©ŸåŸ·è¡Œçš„é‚è¼¯
            print("âš ï¸ å–®æ­¥é©ŸåŸ·è¡ŒåŠŸèƒ½å¾…å¯¦ç¾ï¼Œè«‹ä½¿ç”¨å®Œæ•´æµç¨‹")
        else:
            # åŸ·è¡Œå®Œæ•´æµç¨‹
            success, result = pipeline.run_complete_pipeline(valid_files, args.output)
            
            if success:
                print(f"âœ… æµç¨‹åŸ·è¡ŒæˆåŠŸï¼Œè©³è¦‹: {result}")
                sys.exit(0)
            else:
                print(f"âŒ æµç¨‹åŸ·è¡Œå¤±æ•—: {result}")
                sys.exit(1)
    
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ¶ä¸­æ–·æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ç¨‹åºç•°å¸¸: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()

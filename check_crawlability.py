#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¶²ç«™çˆ¬å–æ¬Šé™æª¢æŸ¥è…³æœ¬ - æª¢æŸ¥domains JSONä¸­çš„ç¶²ç«™robots.txtå’Œçˆ¬å–æ¬Šé™
ä½œè€…: AIåŠ©æ‰‹
æ—¥æœŸ: 2025-07-17
"""

import json
import requests
import argparse
import os
import time
import sys
import signal
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import threading

class RobotsChecker:
    """robots.txtæª¢æŸ¥å™¨"""
    
    def __init__(self, user_agent="*", timeout=10, max_workers=10):
        self.user_agent = user_agent
        self.timeout = timeout
        self.max_workers = max_workers
        
        # é…ç½®session
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; WebCrawler/1.0; +https://example.com/bot)',
            'Accept': 'text/plain,text/html,*/*',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache'
        })
        
        # å˜—è©¦è¨­ç½®é‡è©¦ç­–ç•¥ï¼ˆå¯é¸ï¼‰
        try:
            from requests.adapters import HTTPAdapter
            from urllib3.util.retry import Retry
            
            retry_strategy = Retry(
                total=2,  # ç¸½é‡è©¦æ¬¡æ•¸
                backoff_factor=0.5,  # é‡è©¦é–“éš”
                status_forcelist=[429, 500, 502, 503, 504],  # éœ€è¦é‡è©¦çš„ç‹€æ…‹ç¢¼
                allowed_methods=["GET"]  # åªå°GETè«‹æ±‚é‡è©¦
            )
            
            adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=20, pool_maxsize=20)
            self.session.mount("http://", adapter)
            self.session.mount("https://", adapter)
        except ImportError:
            # å¦‚æœå°å…¥å¤±æ•—ï¼Œä½¿ç”¨åŸºæœ¬é…ç½®
            pass
        
        self.results = {}
        self.lock = threading.Lock()
        self.processed_count = 0
        self.total_count = 0
    
    def check_robots_txt(self, domain):
        """æª¢æŸ¥å–®å€‹åŸŸåçš„robots.txt"""
        result = {
            'domain': domain,
            'robots_url': f"https://{domain}/robots.txt",
            'robots_exists': False,
            'robots_content': None,
            'crawl_allowed': True,
            'crawl_delay': None,
            'disallowed_paths': [],
            'allowed_paths': [],
            'sitemap_urls': [],
            'status_code': None,
            'error': None,
            'response_time': None,
            'last_checked': datetime.now().isoformat()
        }
        
        try:
            # é¦–å…ˆå˜—è©¦HTTPSï¼Œç„¶å¾ŒHTTP
            start_time = time.time()
            
            for protocol in ['https', 'http']:
                robots_url = f"{protocol}://{domain}/robots.txt"
                result['robots_url'] = robots_url
                
                try:
                    # ä½¿ç”¨æ›´çŸ­çš„é€£æ¥è¶…æ™‚å’Œè®€å–è¶…æ™‚
                    response = self.session.get(
                        robots_url, 
                        timeout=(5, self.timeout),  # (é€£æ¥è¶…æ™‚, è®€å–è¶…æ™‚)
                        allow_redirects=True,
                        stream=False
                    )
                    
                    result['status_code'] = response.status_code
                    result['response_time'] = round(time.time() - start_time, 2)
                    
                    if response.status_code == 200:
                        result['robots_exists'] = True
                        # é™åˆ¶å…§å®¹å¤§å°ï¼Œé¿å…è™•ç†éå¤§çš„robots.txt
                        content = response.text[:50000]  # æœ€å¤š50KB
                        result['robots_content'] = content
                        
                        # è§£ærobots.txt
                        self._parse_robots_content(result, robots_url, content)
                        break
                        
                    elif response.status_code == 404:
                        # 404è¡¨ç¤ºæ²’æœ‰robots.txtï¼Œé»˜èªå…è¨±çˆ¬å–
                        result['crawl_allowed'] = True
                        break
                    elif response.status_code in [403, 401]:
                        # æ¬Šé™å•é¡Œï¼Œä¿å®ˆè™•ç†
                        result['error'] = f"æ¬Šé™è¢«æ‹’: {response.status_code}"
                        result['crawl_allowed'] = False
                        break
                    
                except requests.exceptions.Timeout:
                    if protocol == 'http':  # å¦‚æœHTTPä¹Ÿè¶…æ™‚
                        result['error'] = f"é€£æ¥è¶…æ™‚ (>{self.timeout}ç§’)"
                        result['crawl_allowed'] = False
                    continue
                    
                except requests.exceptions.ConnectionError as e:
                    if protocol == 'http':  # å¦‚æœHTTPä¹Ÿé€£æ¥å¤±æ•—
                        result['error'] = f"é€£æ¥å¤±æ•—: {str(e)[:100]}"
                        result['crawl_allowed'] = False
                    continue
                    
                except requests.exceptions.RequestException as e:
                    if protocol == 'http':  # å¦‚æœHTTPä¹Ÿå¤±æ•—
                        result['error'] = f"è«‹æ±‚å¤±æ•—: {str(e)[:100]}"
                        result['crawl_allowed'] = False
                    continue
        
        except Exception as e:
            result['error'] = f"æœªçŸ¥éŒ¯èª¤: {str(e)[:100]}"
            result['crawl_allowed'] = False
        
        return result
    
    def _parse_robots_content(self, result, robots_url, content):
        """è§£ærobots.txtå…§å®¹"""
        try:
            rp = RobotFileParser()
            rp.set_url(robots_url)
            rp.read()
            
            # æª¢æŸ¥æ˜¯å¦å…è¨±çˆ¬å–æ ¹è·¯å¾‘
            result['crawl_allowed'] = rp.can_fetch(self.user_agent, "/")
            
            # è§£æå…§å®¹ä»¥ç²å–æ›´å¤šä¿¡æ¯
            lines = content.strip().split('\n')
            current_user_agent = None
            
            for line in lines:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                if line.lower().startswith('user-agent:'):
                    current_user_agent = line.split(':', 1)[1].strip()
                
                elif line.lower().startswith('disallow:') and (
                    current_user_agent == '*' or 
                    current_user_agent == self.user_agent
                ):
                    path = line.split(':', 1)[1].strip()
                    if path:
                        result['disallowed_paths'].append(path)
                
                elif line.lower().startswith('allow:') and (
                    current_user_agent == '*' or 
                    current_user_agent == self.user_agent
                ):
                    path = line.split(':', 1)[1].strip()
                    if path:
                        result['allowed_paths'].append(path)
                
                elif line.lower().startswith('crawl-delay:') and (
                    current_user_agent == '*' or 
                    current_user_agent == self.user_agent
                ):
                    try:
                        delay = float(line.split(':', 1)[1].strip())
                        result['crawl_delay'] = delay
                    except ValueError:
                        pass
                
                elif line.lower().startswith('sitemap:'):
                    sitemap_url = line.split(':', 1)[1].strip()
                    if sitemap_url:
                        result['sitemap_urls'].append(sitemap_url)
        
        except Exception as e:
            result['error'] = f"robots.txtè§£æéŒ¯èª¤: {str(e)}"
    
    def check_domains_batch(self, domains, verbose=True):
        """æ‰¹é‡æª¢æŸ¥åŸŸååˆ—è¡¨ - æ”¹é€²ç‰ˆæœ¬ï¼Œé˜²æ­¢å¡ä½"""
        self.total_count = len(domains)
        self.processed_count = 0
        start_time = time.time()
        
        if verbose:
            print(f"ğŸ¤– é–‹å§‹æª¢æŸ¥ {len(domains)} å€‹åŸŸåçš„robots.txt...")
        
        # åˆ†æ‰¹è™•ç†ï¼Œæ¯æ‰¹æœ€å¤š100å€‹åŸŸå
        batch_size = min(100, max(10, len(domains) // 10))
        
        for batch_start in range(0, len(domains), batch_size):
            batch_end = min(batch_start + batch_size, len(domains))
            batch_domains = domains[batch_start:batch_end]
            
            if verbose:
                print(f"ğŸ“¦ è™•ç†æ‰¹æ¬¡: {batch_start//batch_size + 1}/{(len(domains)-1)//batch_size + 1} "
                      f"({len(batch_domains)} å€‹åŸŸå)")
            
            self._process_batch(batch_domains, verbose, start_time)
        
        elapsed_total = time.time() - start_time
        if verbose:
            print(f"âœ… æª¢æŸ¥å®Œæˆï¼è€—æ™‚: {elapsed_total:.1f}ç§’")
            print(f"ğŸ“Š æˆåŠŸè™•ç†: {len([r for r in self.results.values() if not r.get('error')])}")
            print(f"âŒ è™•ç†å¤±æ•—: {len([r for r in self.results.values() if r.get('error')])}")
        
        return self.results
    
    def _process_batch(self, batch_domains, verbose, start_time):
        """è™•ç†å–®å€‹æ‰¹æ¬¡"""
        batch_timeout = len(batch_domains) * (self.timeout + 2)  # æ¯å€‹åŸŸåé¡å¤–2ç§’ç·©è¡
        
        with ThreadPoolExecutor(max_workers=min(self.max_workers, len(batch_domains))) as executor:
            # æäº¤æ‰¹æ¬¡ä»»å‹™
            future_to_domain = {
                executor.submit(self.check_robots_txt, domain): domain 
                for domain in batch_domains
            }
            
            completed_in_batch = 0
            
            try:
                # ä½¿ç”¨as_completedè™•ç†çµæœï¼Œæ·»åŠ æ‰¹æ¬¡è¶…æ™‚
                for future in as_completed(future_to_domain, timeout=batch_timeout):
                    domain = future_to_domain[future]
                    
                    try:
                        # ç²å–çµæœï¼Œæ·»åŠ å€‹åˆ¥è¶…æ™‚
                        result = future.result(timeout=2)  # å¿«é€Ÿç²å–å·²å®Œæˆçš„çµæœ
                        
                        with self.lock:
                            self.results[domain] = result
                            self.processed_count += 1
                            completed_in_batch += 1
                            
                            if verbose:
                                progress = (self.processed_count / self.total_count) * 100
                                elapsed = time.time() - start_time
                                rate = self.processed_count / elapsed if elapsed > 0 else 0
                                eta = (self.total_count - self.processed_count) / rate if rate > 0 else 0
                                
                                # å¯¦æ™‚é€²åº¦é¡¯ç¤º
                                if self.processed_count % 5 == 0 or progress >= 98:
                                    print(f"  ğŸ“Š é€²åº¦: {self.processed_count}/{self.total_count} "
                                          f"({progress:.1f}%) - é€Ÿåº¦: {rate:.1f}/ç§’ - ETA: {eta:.0f}ç§’")
                    
                    except Exception as e:
                        with self.lock:
                            self.results[domain] = {
                                'domain': domain,
                                'error': str(e),
                                'crawl_allowed': False,
                                'last_checked': datetime.now().isoformat()
                            }
                            self.processed_count += 1
                            completed_in_batch += 1
                            
                            if verbose:
                                print(f"  âŒ {domain}: {str(e)[:50]}")
            
            except Exception as batch_error:
                if verbose:
                    print(f"âš ï¸ æ‰¹æ¬¡è™•ç†ç•°å¸¸: {batch_error}")
                
                # è™•ç†æœªå®Œæˆçš„ä»»å‹™
                for future in future_to_domain:
                    if not future.done():
                        future.cancel()
                        domain = future_to_domain[future]
                        with self.lock:
                            if domain not in self.results:
                                self.results[domain] = {
                                    'domain': domain,
                                    'error': 'æ‰¹æ¬¡è™•ç†è¶…æ™‚æˆ–è¢«å–æ¶ˆ',
                                    'crawl_allowed': False,
                                    'last_checked': datetime.now().isoformat()
                                }
                                self.processed_count += 1

class CrawlabilityAnalyzer:
    """çˆ¬å–èƒ½åŠ›åˆ†æå™¨"""
    
    def __init__(self, robots_results):
        self.robots_results = robots_results
    
    def analyze_crawlability(self):
        """åˆ†æçˆ¬å–èƒ½åŠ›çµ±è¨ˆ"""
        stats = {
            'total_domains': len(self.robots_results),
            'crawlable_domains': 0,
            'non_crawlable_domains': 0,
            'robots_exists_count': 0,
            'avg_crawl_delay': 0,
            'domains_with_sitemap': 0,
            'error_count': 0,
            'tld_analysis': {},
            'crawl_delay_distribution': {},
            'status_code_distribution': {}
        }
        
        crawl_delays = []
        
        for domain, result in self.robots_results.items():
            # åŸºæœ¬çµ±è¨ˆ
            if result.get('crawl_allowed', False):
                stats['crawlable_domains'] += 1
            else:
                stats['non_crawlable_domains'] += 1
            
            if result.get('robots_exists', False):
                stats['robots_exists_count'] += 1
            
            if result.get('sitemap_urls'):
                stats['domains_with_sitemap'] += 1
            
            if result.get('error'):
                stats['error_count'] += 1
            
            # çˆ¬å–å»¶é²åˆ†æ
            if result.get('crawl_delay'):
                crawl_delays.append(result['crawl_delay'])
                
                delay_range = self._get_delay_range(result['crawl_delay'])
                stats['crawl_delay_distribution'][delay_range] = \
                    stats['crawl_delay_distribution'].get(delay_range, 0) + 1
            
            # TLDåˆ†æ
            tld = domain.split('.')[-1] if '.' in domain else 'unknown'
            if tld not in stats['tld_analysis']:
                stats['tld_analysis'][tld] = {
                    'total': 0,
                    'crawlable': 0,
                    'robots_exists': 0
                }
            
            stats['tld_analysis'][tld]['total'] += 1
            if result.get('crawl_allowed', False):
                stats['tld_analysis'][tld]['crawlable'] += 1
            if result.get('robots_exists', False):
                stats['tld_analysis'][tld]['robots_exists'] += 1
            
            # ç‹€æ…‹ç¢¼åˆ†æ
            status_code = result.get('status_code', 'unknown')
            stats['status_code_distribution'][str(status_code)] = \
                stats['status_code_distribution'].get(str(status_code), 0) + 1
        
        # è¨ˆç®—å¹³å‡çˆ¬å–å»¶é²
        if crawl_delays:
            stats['avg_crawl_delay'] = round(sum(crawl_delays) / len(crawl_delays), 2)
        
        return stats
    
    def _get_delay_range(self, delay):
        """ç²å–å»¶é²ç¯„åœæ¨™ç±¤"""
        if delay <= 1:
            return "0-1ç§’"
        elif delay <= 5:
            return "1-5ç§’"
        elif delay <= 10:
            return "5-10ç§’"
        else:
            return "10ç§’ä»¥ä¸Š"
    
    def generate_recommendations(self, stats):
        """ç”Ÿæˆçˆ¬å–å»ºè­°"""
        recommendations = {
            'safe_to_crawl': [],
            'crawl_with_caution': [],
            'do_not_crawl': [],
            'general_advice': []
        }
        
        for domain, result in self.robots_results.items():
            if result.get('error'):
                recommendations['do_not_crawl'].append({
                    'domain': domain,
                    'reason': f"æª¢æŸ¥å¤±æ•—: {result['error']}"
                })
            elif not result.get('crawl_allowed', True):
                recommendations['do_not_crawl'].append({
                    'domain': domain,
                    'reason': "robots.txtæ˜ç¢ºç¦æ­¢çˆ¬å–"
                })
            elif result.get('crawl_delay', 0) > 10:
                recommendations['crawl_with_caution'].append({
                    'domain': domain,
                    'reason': f"è¦æ±‚å»¶é² {result['crawl_delay']} ç§’"
                })
            else:
                recommendations['safe_to_crawl'].append({
                    'domain': domain,
                    'crawl_delay': result.get('crawl_delay', 0)
                })
        
        # ç”Ÿæˆä¸€èˆ¬å»ºè­°
        crawlable_rate = (stats['crawlable_domains'] / stats['total_domains']) * 100
        
        if crawlable_rate > 80:
            recommendations['general_advice'].append("å¤§éƒ¨åˆ†ç¶²ç«™å…è¨±çˆ¬å–ï¼Œæ•´é«”é¢¨éšªè¼ƒä½")
        elif crawlable_rate > 50:
            recommendations['general_advice'].append("ç´„åŠæ•¸ç¶²ç«™å…è¨±çˆ¬å–ï¼Œå»ºè­°ä»”ç´°æª¢æŸ¥robots.txt")
        else:
            recommendations['general_advice'].append("å¤šæ•¸ç¶²ç«™é™åˆ¶çˆ¬å–ï¼Œå»ºè­°è¬¹æ…è™•ç†")
        
        if stats['avg_crawl_delay'] > 5:
            recommendations['general_advice'].append(f"å¹³å‡çˆ¬å–å»¶é² {stats['avg_crawl_delay']} ç§’ï¼Œéœ€è¦è€ƒæ…®çˆ¬å–æ•ˆç‡")
        
        return recommendations

def load_domain_json(file_path):
    """åŠ è¼‰åŸŸåJSONæ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æ ¹æ“šä¸åŒæ ¼å¼æå–åŸŸååˆ—è¡¨
        if 'domains' in data:
            if isinstance(data['domains'], list):
                # simple_listæ ¼å¼
                return data['domains']
            elif isinstance(data['domains'], dict):
                # detailed_statsæ ¼å¼
                return list(data['domains'].keys())
            elif isinstance(data['domains'], list) and data['domains'] and 'domain' in data['domains'][0]:
                # frequency_rankedæ ¼å¼
                return [item['domain'] for item in data['domains']]
        
        return []
    
    except Exception as e:
        print(f"âŒ è¼‰å…¥JSONæ–‡ä»¶å¤±æ•—: {e}")
        return []

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='æª¢æŸ¥åŸŸåçš„robots.txtå’Œçˆ¬å–æ¬Šé™')
    parser.add_argument('domain_json', help='åŒ…å«åŸŸåçš„JSONæ–‡ä»¶')
    parser.add_argument('-o', '--output', default='crawlability_check', help='è¼¸å‡ºç›®éŒ„ (é»˜èª: crawlability_check)')
    parser.add_argument('-u', '--user-agent', default='*', help='User-Agent (é»˜èª: *)')
    parser.add_argument('-t', '--timeout', type=int, default=10, help='è«‹æ±‚è¶…æ™‚æ™‚é–“ (é»˜èª: 10ç§’)')
    parser.add_argument('-w', '--workers', type=int, default=10, help='ä¸¦ç™¼æ•¸é‡ (é»˜èª: 10)')
    parser.add_argument('-v', '--verbose', action='store_true', help='é¡¯ç¤ºè©³ç´°è¼¸å‡º')
    parser.add_argument('--limit', type=int, help='é™åˆ¶æª¢æŸ¥çš„åŸŸåæ•¸é‡ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰')
    
    args = parser.parse_args()
    
    # è¨­ç½®ä¿¡è™Ÿè™•ç†å™¨ï¼Œå…è¨±å„ªé›…ä¸­æ–·
    interrupted = threading.Event()
    results = {}
    
    def signal_handler(signum, frame):
        print(f"\nâš ï¸ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨ä¿å­˜å·²è™•ç†çš„çµæœ...")
        interrupted.set()
    
    signal.signal(signal.SIGINT, signal_handler)
    if hasattr(signal, 'SIGTERM'):
        signal.signal(signal.SIGTERM, signal_handler)
    
    # è¼‰å…¥åŸŸååˆ—è¡¨
    domains = load_domain_json(args.domain_json)
    
    if not domains:
        print("âŒ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åŸŸååˆ—è¡¨")
        sys.exit(1)
    
    if args.limit:
        domains = domains[:args.limit]
        print(f"âš ï¸  é™åˆ¶æª¢æŸ¥å‰ {args.limit} å€‹åŸŸå")
    
    if args.verbose:
        print(f"ğŸ“‚ è¼‰å…¥åŸŸåæ–‡ä»¶: {args.domain_json}")
        print(f"ğŸŒ ç™¼ç¾ {len(domains)} å€‹åŸŸå")
        print(f"âš™ï¸ ä¸¦ç™¼æ•¸: {args.workers}, è¶…æ™‚: {args.timeout}ç§’")
        print(f"ğŸ’¡ æç¤º: æŒ‰ Ctrl+C å¯ä»¥éš¨æ™‚ä¸­æ–·ä¸¦ä¿å­˜å·²è™•ç†çš„çµæœ")
    
    try:
        # åˆå§‹åŒ–æª¢æŸ¥å™¨
        checker = RobotsChecker(
            user_agent=args.user_agent,
            timeout=args.timeout,
            max_workers=args.workers
        )
        
        # åŸ·è¡Œæª¢æŸ¥
        results = checker.check_domains_batch(domains, args.verbose)
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ç”¨æˆ¶ä¸­æ–·æ“ä½œ")
        results = getattr(checker, 'results', {})
    
    except Exception as e:
        print(f"\nâŒ è™•ç†éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
        results = getattr(checker, 'results', {})
    
    # ç¢ºä¿æœ‰çµæœå¯ä»¥åˆ†æ
    if not results:
        print("âŒ æ²’æœ‰ä»»ä½•è™•ç†çµæœ")
        sys.exit(1)
    
    print(f"\nğŸ“Š æº–å‚™åˆ†æ {len(results)} å€‹çµæœ...")
    
    # åˆ†æçµæœ
    analyzer = CrawlabilityAnalyzer(results)
    stats = analyzer.analyze_crawlability()
    recommendations = analyzer.generate_recommendations(stats)
    
    # è¼¸å‡ºçµæœ
    os.makedirs(args.output, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. è©³ç´°çµæœ
    results_file = os.path.join(args.output, f"robots_check_results_{timestamp}.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'total_domains': len(domains),
                'user_agent': args.user_agent,
                'timeout': args.timeout,
                'script_version': '1.0'
            },
            'results': results
        }, f, ensure_ascii=False, indent=2)
    
    # 2. çµ±è¨ˆåˆ†æ
    stats_file = os.path.join(args.output, f"crawlability_stats_{timestamp}.json")
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'analysis_version': '1.0'
            },
            'statistics': stats,
            'recommendations': recommendations
        }, f, ensure_ascii=False, indent=2)
    
    # 3. å¯çˆ¬å–åŸŸååˆ—è¡¨
    crawlable_domains = [
        domain for domain, result in results.items() 
        if result.get('crawl_allowed', False) and not result.get('error')
    ]
    
    crawlable_file = os.path.join(args.output, f"crawlable_domains_{timestamp}.json")
    with open(crawlable_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'total_crawlable': len(crawlable_domains),
                'source_file': args.domain_json
            },
            'crawlable_domains': crawlable_domains
        }, f, ensure_ascii=False, indent=2)
    
    # é¡¯ç¤ºæ‘˜è¦
    print(f"\nğŸ“Š æª¢æŸ¥çµæœæ‘˜è¦:")
    print(f"  ç¸½åŸŸåæ•¸: {stats['total_domains']}")
    print(f"  å¯çˆ¬å–: {stats['crawlable_domains']} ({stats['crawlable_domains']/stats['total_domains']*100:.1f}%)")
    print(f"  ä¸å¯çˆ¬å–: {stats['non_crawlable_domains']} ({stats['non_crawlable_domains']/stats['total_domains']*100:.1f}%)")
    print(f"  æœ‰robots.txt: {stats['robots_exists_count']} ({stats['robots_exists_count']/stats['total_domains']*100:.1f}%)")
    print(f"  æª¢æŸ¥éŒ¯èª¤: {stats['error_count']}")
    
    if stats['avg_crawl_delay'] > 0:
        print(f"  å¹³å‡çˆ¬å–å»¶é²: {stats['avg_crawl_delay']} ç§’")
    
    print(f"\nğŸ“ è¼¸å‡ºæ–‡ä»¶:")
    print(f"  ğŸ“‹ è©³ç´°çµæœ: {results_file}")
    print(f"  ğŸ“Š çµ±è¨ˆåˆ†æ: {stats_file}")
    print(f"  âœ… å¯çˆ¬å–åˆ—è¡¨: {crawlable_file}")
    
    print(f"\nâœ… æª¢æŸ¥å®Œæˆï¼")

if __name__ == "__main__":
    main()

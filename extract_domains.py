#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸŸåæå–è…³æœ¬ - å¾JSONLæ•¸æ“šé›†ä¸­æå–å”¯ä¸€åŸŸåä¸¦ç”ŸæˆJSON
ä½œè€…: AIåŠ©æ‰‹
æ—¥æœŸ: 2025-07-17
"""

import json
import argparse
import os
from urllib.parse import urlparse
from collections import defaultdict, Counter
from datetime import datetime
import sys

class DomainExtractor:
    """æ™ºèƒ½åŸŸåæå–å™¨"""
    
    def __init__(self):
        self.domain_stats = defaultdict(lambda: {
            'count': 0,
            'urls': [],
            'tld': '',
            'subdomain_count': 0,
            'first_seen': None,
            'last_seen': None
        })
        self.processed_count = 0
        self.total_count = 0
    
    def extract_domain(self, url):
        """å¾URLæå–åŸŸå"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # ç§»é™¤ç«¯å£è™Ÿ
            if ':' in domain:
                domain = domain.split(':')[0]
            
            # ç§»é™¤wwwå‰ç¶´ï¼ˆå¯é¸ï¼‰
            if domain.startswith('www.'):
                domain = domain[4:]
            
            return domain if domain else None
        except Exception as e:
            print(f"URLè§£æéŒ¯èª¤ {url}: {e}")
            return None
    
    def extract_tld(self, domain):
        """æå–é ‚ç´šåŸŸå"""
        if not domain or '.' not in domain:
            return ''
        return domain.split('.')[-1]
    
    def count_subdomains(self, domain):
        """è¨ˆç®—å­åŸŸåæ•¸é‡"""
        if not domain:
            return 0
        return domain.count('.')
    
    def load_jsonl_data(self, file_path):
        """å¾JSONLæ–‡ä»¶åŠ è¼‰æ•¸æ“š"""
        data = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if line.strip():
                        try:
                            data.append(json.loads(line))
                        except json.JSONDecodeError as e:
                            print(f"ç¬¬{line_num}è¡ŒJSONè§£æéŒ¯èª¤: {e}")
            return data
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
            return []
        except Exception as e:
            print(f"âŒ è®€å–æ–‡ä»¶æ™‚å‡ºéŒ¯: {e}")
            return []
    
    def process_urls(self, data, verbose=True):
        """è™•ç†æ‰€æœ‰URLä¸¦æå–åŸŸåä¿¡æ¯"""
        if verbose:
            print(f"ğŸ”„ é–‹å§‹è™•ç† {len(data)} æ¢è¨˜éŒ„...")
        
        self.total_count = len(data)
        self.processed_count = 0
        
        for i, record in enumerate(data):
            if verbose and i % 1000 == 0:
                print(f"  è™•ç†é€²åº¦: {i}/{len(data)} ({i/len(data)*100:.1f}%)")
            
            url = record.get('url', '')
            if not url:
                continue
            
            domain = self.extract_domain(url)
            if not domain:
                continue
            
            # æ›´æ–°åŸŸåçµ±è¨ˆ
            stats = self.domain_stats[domain]
            stats['count'] += 1
            stats['tld'] = self.extract_tld(domain)
            stats['subdomain_count'] = self.count_subdomains(domain)
            
            # è¨˜éŒ„URLç¤ºä¾‹ï¼ˆæœ€å¤šä¿å­˜5å€‹ï¼‰
            if len(stats['urls']) < 5:
                stats['urls'].append(url)
            
            # è¨˜éŒ„æ™‚é–“æˆ³
            timestamp = record.get('timestamp') or record.get('date') or datetime.now().isoformat()
            if stats['first_seen'] is None:
                stats['first_seen'] = timestamp
            stats['last_seen'] = timestamp
            
            self.processed_count += 1
        
        if verbose:
            print(f"âœ… è™•ç†å®Œæˆï¼å…±è™•ç† {self.processed_count} å€‹URL")
            print(f"ğŸ“Š ç™¼ç¾ {len(self.domain_stats)} å€‹å”¯ä¸€åŸŸå")
        
        return self.domain_stats

class JSONOutputGenerator:
    """JSONè¼¸å‡ºç”Ÿæˆå™¨"""
    
    def __init__(self, domain_data):
        self.domain_data = domain_data
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    def generate_simple_list(self):
        """ç”Ÿæˆç°¡å–®çš„åŸŸååˆ—è¡¨"""
        domains = list(self.domain_data.keys())
        domains.sort()
        return {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_domains": len(domains),
                "format": "simple_list",
                "script_version": "1.0"
            },
            "domains": domains
        }
    
    def generate_detailed_stats(self):
        """ç”Ÿæˆè©³ç´°çš„åŸŸåçµ±è¨ˆä¿¡æ¯"""
        detailed_data = {}
        
        for domain, stats in self.domain_data.items():
            detailed_data[domain] = {
                "count": stats['count'],
                "tld": stats['tld'],
                "subdomain_count": stats['subdomain_count'],
                "sample_urls": stats['urls'],
                "first_seen": stats['first_seen'],
                "last_seen": stats['last_seen']
            }
        
        return {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_domains": len(detailed_data),
                "total_urls_processed": sum(stats['count'] for stats in self.domain_data.values()),
                "format": "detailed_stats",
                "script_version": "1.0"
            },
            "domains": detailed_data
        }
    
    def generate_frequency_ranked(self):
        """ç”ŸæˆæŒ‰é »ç‡æ’åºçš„åŸŸååˆ—è¡¨"""
        sorted_domains = sorted(
            self.domain_data.items(), 
            key=lambda x: x[1]['count'], 
            reverse=True
        )
        
        total_urls = sum(s['count'] for s in self.domain_data.values())
        ranked_list = []
        
        for rank, (domain, stats) in enumerate(sorted_domains, 1):
            ranked_list.append({
                "rank": rank,
                "domain": domain,
                "count": stats['count'],
                "percentage": round(stats['count'] / total_urls * 100, 2),
                "tld": stats['tld']
            })
        
        return {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_domains": len(ranked_list),
                "ranking_criteria": "url_frequency",
                "format": "frequency_ranked",
                "script_version": "1.0"
            },
            "domains": ranked_list
        }
    
    def save_json(self, data, filename, output_dir):
        """ä¿å­˜å–®å€‹JSONæ–‡ä»¶"""
        os.makedirs(output_dir, exist_ok=True)
        filepath = os.path.join(output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return filepath

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='å¾JSONLæ•¸æ“šé›†ä¸­æå–åŸŸåä¸¦ç”ŸæˆJSON')
    parser.add_argument('input_files', nargs='+', help='è¼¸å…¥çš„JSONLæ–‡ä»¶è·¯å¾‘')
    parser.add_argument('-o', '--output', default='domain_extracts', help='è¼¸å‡ºç›®éŒ„ (é»˜èª: domain_extracts)')
    parser.add_argument('-f', '--format', choices=['simple', 'detailed', 'ranked', 'all'], 
                       default='all', help='è¼¸å‡ºæ ¼å¼ (é»˜èª: all)')
    parser.add_argument('-v', '--verbose', action='store_true', help='é¡¯ç¤ºè©³ç´°è¼¸å‡º')
    parser.add_argument('--no-www', action='store_true', help='ç§»é™¤wwwå‰ç¶´')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æå–å™¨
    extractor = DomainExtractor()
    
    # åŠ è¼‰æ‰€æœ‰æ•¸æ“šæ–‡ä»¶
    all_data = []
    for file_path in args.input_files:
        if os.path.exists(file_path):
            if args.verbose:
                print(f"ğŸ“‚ åŠ è¼‰æ–‡ä»¶: {file_path}")
            
            file_data = extractor.load_jsonl_data(file_path)
            if args.verbose:
                print(f"  â””â”€ æˆåŠŸåŠ è¼‰ {len(file_data)} æ¢è¨˜éŒ„")
            all_data.extend(file_data)
        else:
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    if not all_data:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ•¸æ“š")
        sys.exit(1)
    
    if args.verbose:
        print(f"\nâœ… ç¸½å…±åŠ è¼‰äº† {len(all_data)} æ¢è¨˜éŒ„")
    
    # è™•ç†æ•¸æ“š
    domain_data = extractor.process_urls(all_data, args.verbose)
    
    if not domain_data:
        print("âŒ æ²’æœ‰æå–åˆ°ä»»ä½•åŸŸå")
        sys.exit(1)
    
    # ç”Ÿæˆè¼¸å‡º
    generator = JSONOutputGenerator(domain_data)
    
    # é¡¯ç¤ºçµ±è¨ˆæ‘˜è¦
    if args.verbose:
        print(f"\nğŸ“Š åŸŸåçµ±è¨ˆæ‘˜è¦:")
        print(f"  å”¯ä¸€åŸŸåç¸½æ•¸: {len(domain_data):,}")
        
        # é¡¯ç¤ºå‰5å€‹æœ€é »ç¹åŸŸå
        top_domains = sorted(domain_data.items(), key=lambda x: x[1]['count'], reverse=True)[:5]
        print(f"\nğŸ† å‰5å€‹æœ€é »ç¹åŸŸå:")
        for i, (domain, stats) in enumerate(top_domains, 1):
            print(f"  {i}. {domain}: {stats['count']} æ¬¡")
        
        # TLDçµ±è¨ˆ
        tld_counter = Counter(stats['tld'] for stats in domain_data.values())
        print(f"\nğŸŒ é ‚ç´šåŸŸååˆ†å¸ƒ (å‰3):")
        for tld, count in tld_counter.most_common(3):
            print(f"  .{tld}: {count} å€‹åŸŸå")
    
    # ä¿å­˜æ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    saved_files = []
    
    if args.format in ['simple', 'all']:
        data = generator.generate_simple_list()
        filename = f"domains_simple_{timestamp}.json"
        filepath = generator.save_json(data, filename, args.output)
        saved_files.append(filepath)
        if args.verbose:
            print(f"ğŸ’¾ å·²ä¿å­˜ç°¡å–®åˆ—è¡¨: {filepath}")
    
    if args.format in ['detailed', 'all']:
        data = generator.generate_detailed_stats()
        filename = f"domains_detailed_{timestamp}.json"
        filepath = generator.save_json(data, filename, args.output)
        saved_files.append(filepath)
        if args.verbose:
            print(f"ğŸ’¾ å·²ä¿å­˜è©³ç´°çµ±è¨ˆ: {filepath}")
    
    if args.format in ['ranked', 'all']:
        data = generator.generate_frequency_ranked()
        filename = f"domains_ranked_{timestamp}.json"
        filepath = generator.save_json(data, filename, args.output)
        saved_files.append(filepath)
        if args.verbose:
            print(f"ğŸ’¾ å·²ä¿å­˜é »ç‡æ’åº: {filepath}")
    
    # è¼¸å‡ºçµæœæ‘˜è¦
    print(f"\nâœ… å®Œæˆï¼")
    print(f"ğŸ“Š è™•ç†äº† {len(all_data)} æ¢è¨˜éŒ„")
    print(f"ğŸŒ ç™¼ç¾ {len(domain_data)} å€‹å”¯ä¸€åŸŸå")
    print(f"ğŸ“ ç”Ÿæˆäº† {len(saved_files)} å€‹JSONæ–‡ä»¶")
    print(f"ğŸ“‚ è¼¸å‡ºç›®éŒ„: {args.output}")

if __name__ == "__main__":
    main()

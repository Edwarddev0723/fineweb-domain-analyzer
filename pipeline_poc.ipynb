{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdf64cba",
   "metadata": {},
   "source": [
    "# URLåŸŸåæå–å’ŒJSONæ•´ç†å·¥å…·\n",
    "\n",
    "æ­¤notebookç”¨æ–¼å¾WARC/JSONLæ•¸æ“šä¸­æå–å”¯ä¸€åŸŸåä¸¦æ•´ç†æˆJSONæ ¼å¼ï¼Œæ–¹ä¾¿å¾ŒçºŒåˆ†æå’Œä½¿ç”¨ã€‚\n",
    "\n",
    "## åŠŸèƒ½ç‰¹é»\n",
    "- å¾URLä¸­æå–å®Œæ•´åŸŸå\n",
    "- çµ±è¨ˆæ¯å€‹åŸŸåçš„å‡ºç¾é »æ¬¡\n",
    "- ç”Ÿæˆçµæ§‹åŒ–çš„JSONè¼¸å‡º\n",
    "- æ”¯æŒå¤šç¨®è¼¸å‡ºæ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "587ae8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š å·²å°å…¥æ‰€éœ€åº«\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "print(\"ğŸ“š å·²å°å…¥æ‰€éœ€åº«\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "754faf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” é–‹å§‹åŠ è¼‰æ•¸æ“šæ–‡ä»¶...\n",
      "åŠ è¼‰æ–‡ä»¶: output_all\\extracted_content_CC-MAIN-20240612140424-20240612170424-00000.warc.jsonl\n",
      "  â””â”€ æˆåŠŸåŠ è¼‰ 3184 æ¢è¨˜éŒ„\n",
      "åŠ è¼‰æ–‡ä»¶: fineweb-zhtw\\data\\output_high_quality\\clean_traditional_chinese.jsonl\n",
      "  â””â”€ æˆåŠŸåŠ è¼‰ 8 æ¢è¨˜éŒ„\n",
      "\n",
      "âœ… ç¸½å…±åŠ è¼‰äº† 3192 æ¢è¨˜éŒ„\n"
     ]
    }
   ],
   "source": [
    "# æ•¸æ“šåŠ è¼‰å‡½æ•¸\n",
    "def load_jsonl_data(file_path):\n",
    "    \"\"\"å¾JSONLæ–‡ä»¶åŠ è¼‰æ•¸æ“š\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"ç¬¬{line_num}è¡ŒJSONè§£æéŒ¯èª¤: {e}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"è®€å–æ–‡ä»¶æ™‚å‡ºéŒ¯: {e}\")\n",
    "        return []\n",
    "\n",
    "# æŒ‡å®šæ•¸æ“šæ–‡ä»¶è·¯å¾„\n",
    "data_files = [\n",
    "    r\"output_all\\extracted_content_CC-MAIN-20240612140424-20240612170424-00000.warc.jsonl\",\n",
    "    r\"fineweb-zhtw\\data\\output_high_quality\\clean_traditional_chinese.jsonl\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ” é–‹å§‹åŠ è¼‰æ•¸æ“šæ–‡ä»¶...\")\n",
    "\n",
    "all_data = []\n",
    "for file_path in data_files:\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"åŠ è¼‰æ–‡ä»¶: {file_path}\")\n",
    "        file_data = load_jsonl_data(file_path)\n",
    "        print(f\"  â””â”€ æˆåŠŸåŠ è¼‰ {len(file_data)} æ¢è¨˜éŒ„\")\n",
    "        all_data.extend(file_data)\n",
    "    else:\n",
    "        print(f\"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")\n",
    "\n",
    "print(f\"\\nâœ… ç¸½å…±åŠ è¼‰äº† {len(all_data)} æ¢è¨˜éŒ„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4349277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ é–‹å§‹è™•ç†URLä¸¦æå–åŸŸå...\n",
      "  è™•ç†é€²åº¦: 0/3192 (0.0%)\n",
      "  è™•ç†é€²åº¦: 1000/3192 (31.3%)\n",
      "  è™•ç†é€²åº¦: 2000/3192 (62.7%)\n",
      "  è™•ç†é€²åº¦: 3000/3192 (94.0%)\n",
      "âœ… è™•ç†å®Œæˆï¼å…±è™•ç† 3192 å€‹URL\n",
      "ğŸ“Š ç™¼ç¾ 2954 å€‹å”¯ä¸€åŸŸå\n"
     ]
    }
   ],
   "source": [
    "# åŸŸåæå–å’Œè™•ç†é¡\n",
    "class DomainExtractor:\n",
    "    \"\"\"æ™ºèƒ½åŸŸåæå–å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.domain_stats = defaultdict(lambda: {\n",
    "            'count': 0,\n",
    "            'urls': [],\n",
    "            'tld': '',\n",
    "            'subdomain_count': 0,\n",
    "            'first_seen': None,\n",
    "            'last_seen': None\n",
    "        })\n",
    "    \n",
    "    def extract_domain(self, url):\n",
    "        \"\"\"å¾URLæå–åŸŸå\"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            domain = parsed.netloc.lower()\n",
    "            \n",
    "            # ç§»é™¤ç«¯å£è™Ÿ\n",
    "            if ':' in domain:\n",
    "                domain = domain.split(':')[0]\n",
    "            \n",
    "            # ç§»é™¤wwwå‰ç¶´ï¼ˆå¯é¸ï¼‰\n",
    "            if domain.startswith('www.'):\n",
    "                domain = domain[4:]\n",
    "            \n",
    "            return domain\n",
    "        except Exception as e:\n",
    "            print(f\"URLè§£æéŒ¯èª¤ {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_tld(self, domain):\n",
    "        \"\"\"æå–é ‚ç´šåŸŸå\"\"\"\n",
    "        if not domain or '.' not in domain:\n",
    "            return ''\n",
    "        return domain.split('.')[-1]\n",
    "    \n",
    "    def count_subdomains(self, domain):\n",
    "        \"\"\"è¨ˆç®—å­åŸŸåæ•¸é‡\"\"\"\n",
    "        if not domain:\n",
    "            return 0\n",
    "        return domain.count('.')\n",
    "    \n",
    "    def process_urls(self, data):\n",
    "        \"\"\"è™•ç†æ‰€æœ‰URLä¸¦æå–åŸŸåä¿¡æ¯\"\"\"\n",
    "        print(\"ğŸ”„ é–‹å§‹è™•ç†URLä¸¦æå–åŸŸå...\")\n",
    "        \n",
    "        processed_count = 0\n",
    "        for i, record in enumerate(data):\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"  è™•ç†é€²åº¦: {i}/{len(data)} ({i/len(data)*100:.1f}%)\")\n",
    "            \n",
    "            url = record.get('url', '')\n",
    "            if not url:\n",
    "                continue\n",
    "            \n",
    "            domain = self.extract_domain(url)\n",
    "            if not domain:\n",
    "                continue\n",
    "            \n",
    "            # æ›´æ–°åŸŸåçµ±è¨ˆ\n",
    "            stats = self.domain_stats[domain]\n",
    "            stats['count'] += 1\n",
    "            stats['tld'] = self.extract_tld(domain)\n",
    "            stats['subdomain_count'] = self.count_subdomains(domain)\n",
    "            \n",
    "            # è¨˜éŒ„URLç¤ºä¾‹ï¼ˆæœ€å¤šä¿å­˜5å€‹ï¼‰\n",
    "            if len(stats['urls']) < 5:\n",
    "                stats['urls'].append(url)\n",
    "            \n",
    "            # è¨˜éŒ„æ™‚é–“æˆ³ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰\n",
    "            timestamp = record.get('timestamp') or record.get('date') or datetime.now().isoformat()\n",
    "            if stats['first_seen'] is None:\n",
    "                stats['first_seen'] = timestamp\n",
    "            stats['last_seen'] = timestamp\n",
    "            \n",
    "            processed_count += 1\n",
    "        \n",
    "        print(f\"âœ… è™•ç†å®Œæˆï¼å…±è™•ç† {processed_count} å€‹URL\")\n",
    "        print(f\"ğŸ“Š ç™¼ç¾ {len(self.domain_stats)} å€‹å”¯ä¸€åŸŸå\")\n",
    "        \n",
    "        return self.domain_stats\n",
    "\n",
    "# åˆå§‹åŒ–åŸŸåæå–å™¨\n",
    "extractor = DomainExtractor()\n",
    "\n",
    "# è™•ç†æ•¸æ“š\n",
    "if all_data:\n",
    "    domain_data = extractor.process_urls(all_data)\n",
    "else:\n",
    "    print(\"âš ï¸ æ²’æœ‰æ•¸æ“šå¯è™•ç†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "842fbf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ é–‹å§‹ç”ŸæˆJSONè¼¸å‡º...\n",
      "\n",
      "ğŸ“Š åŸŸåçµ±è¨ˆæ‘˜è¦:\n",
      "  å”¯ä¸€åŸŸåç¸½æ•¸: 2,954\n",
      "\n",
      "ğŸ† å‰10å€‹æœ€é »ç¹åŸŸå:\n",
      "   1. cujasweb.univ-paris1.fr: 5 æ¬¡\n",
      "   2. licitacoeseb.9rm.eb.mil.br: 4 æ¬¡\n",
      "   3. luna.library.cmu.edu: 4 æ¬¡\n",
      "   4. siganus.php.xdomain.jp: 4 æ¬¡\n",
      "   5. imagemagick.org: 4 æ¬¡\n",
      "   6. business.kanerepublican.com: 3 æ¬¡\n",
      "   7. collections.artmuseum.utoronto.ca: 3 æ¬¡\n",
      "   8. forum.artofwar.net.ru: 3 æ¬¡\n",
      "   9. ftp.cpan.org: 3 æ¬¡\n",
      "  10. ftp.mozilla.org: 3 æ¬¡\n",
      "\n",
      "ğŸŒ é ‚ç´šåŸŸååˆ†å¸ƒ (å‰5):\n",
      "  .com: 1421 å€‹åŸŸå\n",
      "  .ru: 185 å€‹åŸŸå\n",
      "  .org: 138 å€‹åŸŸå\n",
      "  .net: 135 å€‹åŸŸå\n",
      "  .cn: 113 å€‹åŸŸå\n",
      "\n",
      "ğŸ’¾ æ­£åœ¨ä¿å­˜JSONæ–‡ä»¶...\n",
      "ğŸ’¾ å·²ä¿å­˜: domain_extracts\\simple_list_20250717_162329.json\n",
      "ğŸ’¾ å·²ä¿å­˜: domain_extracts\\detailed_stats_20250717_162329.json\n",
      "ğŸ’¾ å·²ä¿å­˜: domain_extracts\\frequency_ranked_20250717_162329.json\n",
      "ğŸ’¾ å·²ä¿å­˜: domain_extracts\\tld_grouped_20250717_162329.json\n",
      "\n",
      "âœ… å®Œæˆï¼å…±ç”Ÿæˆ 4 å€‹JSONæ–‡ä»¶\n"
     ]
    }
   ],
   "source": [
    "# JSONè¼¸å‡ºç”Ÿæˆå™¨\n",
    "class JSONOutputGenerator:\n",
    "    \"\"\"ç”Ÿæˆå¤šç¨®æ ¼å¼çš„JSONè¼¸å‡º\"\"\"\n",
    "    \n",
    "    def __init__(self, domain_data):\n",
    "        self.domain_data = domain_data\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    def generate_simple_list(self):\n",
    "        \"\"\"ç”Ÿæˆç°¡å–®çš„åŸŸååˆ—è¡¨\"\"\"\n",
    "        domains = list(self.domain_data.keys())\n",
    "        domains.sort()\n",
    "        return {\n",
    "            \"metadata\": {\n",
    "                \"generated_at\": datetime.now().isoformat(),\n",
    "                \"total_domains\": len(domains),\n",
    "                \"format\": \"simple_list\"\n",
    "            },\n",
    "            \"domains\": domains\n",
    "        }\n",
    "    \n",
    "    def generate_detailed_stats(self):\n",
    "        \"\"\"ç”Ÿæˆè©³ç´°çš„åŸŸåçµ±è¨ˆä¿¡æ¯\"\"\"\n",
    "        detailed_data = {}\n",
    "        \n",
    "        for domain, stats in self.domain_data.items():\n",
    "            detailed_data[domain] = {\n",
    "                \"count\": stats['count'],\n",
    "                \"tld\": stats['tld'],\n",
    "                \"subdomain_count\": stats['subdomain_count'],\n",
    "                \"sample_urls\": stats['urls'],\n",
    "                \"first_seen\": stats['first_seen'],\n",
    "                \"last_seen\": stats['last_seen']\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"metadata\": {\n",
    "                \"generated_at\": datetime.now().isoformat(),\n",
    "                \"total_domains\": len(detailed_data),\n",
    "                \"total_urls_processed\": sum(stats['count'] for stats in self.domain_data.values()),\n",
    "                \"format\": \"detailed_stats\"\n",
    "            },\n",
    "            \"domains\": detailed_data\n",
    "        }\n",
    "    \n",
    "    def generate_frequency_ranked(self):\n",
    "        \"\"\"ç”ŸæˆæŒ‰é »ç‡æ’åºçš„åŸŸååˆ—è¡¨\"\"\"\n",
    "        sorted_domains = sorted(\n",
    "            self.domain_data.items(), \n",
    "            key=lambda x: x[1]['count'], \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        ranked_list = []\n",
    "        for rank, (domain, stats) in enumerate(sorted_domains, 1):\n",
    "            ranked_list.append({\n",
    "                \"rank\": rank,\n",
    "                \"domain\": domain,\n",
    "                \"count\": stats['count'],\n",
    "                \"percentage\": round(stats['count'] / sum(s['count'] for s in self.domain_data.values()) * 100, 2),\n",
    "                \"tld\": stats['tld']\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"metadata\": {\n",
    "                \"generated_at\": datetime.now().isoformat(),\n",
    "                \"total_domains\": len(ranked_list),\n",
    "                \"ranking_criteria\": \"url_frequency\",\n",
    "                \"format\": \"frequency_ranked\"\n",
    "            },\n",
    "            \"domains\": ranked_list\n",
    "        }\n",
    "    \n",
    "    def generate_tld_grouped(self):\n",
    "        \"\"\"æŒ‰é ‚ç´šåŸŸååˆ†çµ„çš„åŸŸååˆ—è¡¨\"\"\"\n",
    "        tld_groups = defaultdict(list)\n",
    "        \n",
    "        for domain, stats in self.domain_data.items():\n",
    "            tld = stats['tld'] or 'no_tld'\n",
    "            tld_groups[tld].append({\n",
    "                \"domain\": domain,\n",
    "                \"count\": stats['count']\n",
    "            })\n",
    "        \n",
    "        # å°æ¯å€‹TLDçµ„å…§çš„åŸŸåæŒ‰é »ç‡æ’åº\n",
    "        for tld in tld_groups:\n",
    "            tld_groups[tld].sort(key=lambda x: x['count'], reverse=True)\n",
    "        \n",
    "        return {\n",
    "            \"metadata\": {\n",
    "                \"generated_at\": datetime.now().isoformat(),\n",
    "                \"total_domains\": len(self.domain_data),\n",
    "                \"total_tlds\": len(tld_groups),\n",
    "                \"format\": \"tld_grouped\"\n",
    "            },\n",
    "            \"tld_groups\": dict(tld_groups)\n",
    "        }\n",
    "    \n",
    "    def save_all_formats(self, output_dir=\"domain_extracts\"):\n",
    "        \"\"\"ä¿å­˜æ‰€æœ‰æ ¼å¼çš„JSONæ–‡ä»¶\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        formats = {\n",
    "            \"simple_list\": self.generate_simple_list(),\n",
    "            \"detailed_stats\": self.generate_detailed_stats(),\n",
    "            \"frequency_ranked\": self.generate_frequency_ranked(),\n",
    "            \"tld_grouped\": self.generate_tld_grouped()\n",
    "        }\n",
    "        \n",
    "        saved_files = []\n",
    "        for format_name, data in formats.items():\n",
    "            filename = f\"{format_name}_{self.timestamp}.json\"\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            \n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            saved_files.append(filepath)\n",
    "            print(f\"ğŸ’¾ å·²ä¿å­˜: {filepath}\")\n",
    "        \n",
    "        return saved_files\n",
    "\n",
    "# ç”ŸæˆJSONè¼¸å‡º\n",
    "if 'domain_data' in locals() and domain_data:\n",
    "    print(\"ğŸ“„ é–‹å§‹ç”ŸæˆJSONè¼¸å‡º...\")\n",
    "    \n",
    "    generator = JSONOutputGenerator(domain_data)\n",
    "    \n",
    "    # é¡¯ç¤ºçµ±è¨ˆæ‘˜è¦\n",
    "    print(f\"\\nğŸ“Š åŸŸåçµ±è¨ˆæ‘˜è¦:\")\n",
    "    print(f\"  å”¯ä¸€åŸŸåç¸½æ•¸: {len(domain_data):,}\")\n",
    "    \n",
    "    # æŒ‰é »ç‡é¡¯ç¤ºå‰10å€‹åŸŸå\n",
    "    top_domains = sorted(domain_data.items(), key=lambda x: x[1]['count'], reverse=True)[:10]\n",
    "    print(f\"\\nğŸ† å‰10å€‹æœ€é »ç¹åŸŸå:\")\n",
    "    for i, (domain, stats) in enumerate(top_domains, 1):\n",
    "        print(f\"  {i:2d}. {domain}: {stats['count']} æ¬¡\")\n",
    "    \n",
    "    # TLDçµ±è¨ˆ\n",
    "    tld_counter = Counter(stats['tld'] for stats in domain_data.values())\n",
    "    print(f\"\\nğŸŒ é ‚ç´šåŸŸååˆ†å¸ƒ (å‰5):\")\n",
    "    for tld, count in tld_counter.most_common(5):\n",
    "        print(f\"  .{tld}: {count} å€‹åŸŸå\")\n",
    "    \n",
    "    # ä¿å­˜æ‰€æœ‰æ ¼å¼\n",
    "    print(f\"\\nğŸ’¾ æ­£åœ¨ä¿å­˜JSONæ–‡ä»¶...\")\n",
    "    saved_files = generator.save_all_formats()\n",
    "    \n",
    "    print(f\"\\nâœ… å®Œæˆï¼å…±ç”Ÿæˆ {len(saved_files)} å€‹JSONæ–‡ä»¶\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ æ²’æœ‰åŸŸåæ•¸æ“šå¯ç”ŸæˆJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "039e52c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ è«‹å…ˆé‹è¡ŒåŸŸåæå–å’ŒJSONç”Ÿæˆ\n",
      "\n",
      "ğŸ“ ç”Ÿæˆçš„JSONæ–‡ä»¶ç”¨é€”èªªæ˜:\n",
      "==================================================\n",
      "1ï¸âƒ£  simple_list.json:\n",
      "   â””â”€ ç´”åŸŸååˆ—è¡¨ï¼Œé©åˆå¿«é€ŸæŸ¥çœ‹å’Œå°å…¥å…¶ä»–å·¥å…·\n",
      "2ï¸âƒ£  detailed_stats.json:\n",
      "   â””â”€ å®Œæ•´çµ±è¨ˆä¿¡æ¯ï¼ŒåŒ…å«URLç¤ºä¾‹ã€å‡ºç¾æ¬¡æ•¸ç­‰\n",
      "3ï¸âƒ£  frequency_ranked.json:\n",
      "   â””â”€ æŒ‰ç†±é–€ç¨‹åº¦æ’åºï¼Œæ–¹ä¾¿è­˜åˆ¥ä¸»è¦åŸŸåä¾†æº\n",
      "4ï¸âƒ£  tld_grouped.json:\n",
      "   â””â”€ æŒ‰ç¶²åŸŸé¡å‹åˆ†çµ„ï¼Œä¾¿æ–¼åˆ†æåŸŸååˆ†å¸ƒç‰¹å¾µ\n",
      "\n",
      "ğŸ’¡ ä½¿ç”¨å»ºè­°:\n",
      "  â€¢ ç”¨æ–¼ç™½åå–®/é»‘åå–®ç®¡ç†\n",
      "  â€¢ åŸŸåä¿¡è­½åˆ†æ\n",
      "  â€¢ æ•¸æ“šä¾†æºè¿½è¸ª\n",
      "  â€¢ ç‰ˆæ¬Šé¢¨éšªè©•ä¼°\n",
      "  â€¢ ç¶²ç«™åˆ†é¡å’Œéæ¿¾\n"
     ]
    }
   ],
   "source": [
    "# JSONæ•¸æ“šé è¦½å’Œé©—è­‰\n",
    "def preview_json_formats():\n",
    "    \"\"\"é è¦½ä¸åŒæ ¼å¼çš„JSONçµæ§‹\"\"\"\n",
    "    if 'generator' not in locals() or not domain_data:\n",
    "        print(\"âš ï¸ è«‹å…ˆé‹è¡ŒåŸŸåæå–å’ŒJSONç”Ÿæˆ\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ” JSONæ ¼å¼é è¦½:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. ç°¡å–®åˆ—è¡¨æ ¼å¼é è¦½\n",
    "    simple_data = generator.generate_simple_list()\n",
    "    print(\"ğŸ“‹ 1. ç°¡å–®åˆ—è¡¨æ ¼å¼ (simple_list.json):\")\n",
    "    print(f\"  â””â”€ åŒ…å« {simple_data['metadata']['total_domains']} å€‹åŸŸå\")\n",
    "    print(\"  â””â”€ çµæ§‹é è¦½:\")\n",
    "    preview_simple = {\n",
    "        \"metadata\": simple_data['metadata'],\n",
    "        \"domains\": simple_data['domains'][:3] + [\"...\"] if len(simple_data['domains']) > 3 else simple_data['domains']\n",
    "    }\n",
    "    print(json.dumps(preview_simple, ensure_ascii=False, indent=4))\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    \n",
    "    # 2. è©³ç´°çµ±è¨ˆæ ¼å¼é è¦½\n",
    "    detailed_data = generator.generate_detailed_stats()\n",
    "    print(\"ğŸ“Š 2. è©³ç´°çµ±è¨ˆæ ¼å¼ (detailed_stats.json):\")\n",
    "    print(f\"  â””â”€ åŒ…å«è©³ç´°çµ±è¨ˆä¿¡æ¯\")\n",
    "    print(\"  â””â”€ çµæ§‹é è¦½:\")\n",
    "    first_domain = list(detailed_data['domains'].keys())[0]\n",
    "    preview_detailed = {\n",
    "        \"metadata\": detailed_data['metadata'],\n",
    "        \"domains\": {\n",
    "            first_domain: detailed_data['domains'][first_domain],\n",
    "            \"...\": \"æ›´å¤šåŸŸåæ•¸æ“š\"\n",
    "        }\n",
    "    }\n",
    "    print(json.dumps(preview_detailed, ensure_ascii=False, indent=4))\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    \n",
    "    # 3. é »ç‡æ’åºæ ¼å¼é è¦½\n",
    "    ranked_data = generator.generate_frequency_ranked()\n",
    "    print(\"ğŸ† 3. é »ç‡æ’åºæ ¼å¼ (frequency_ranked.json):\")\n",
    "    print(f\"  â””â”€ æŒ‰URLå‡ºç¾é »ç‡æ’åº\")\n",
    "    print(\"  â””â”€ çµæ§‹é è¦½:\")\n",
    "    preview_ranked = {\n",
    "        \"metadata\": ranked_data['metadata'],\n",
    "        \"domains\": ranked_data['domains'][:3] + [{\"...\": \"æ›´å¤šæ’åºæ•¸æ“š\"}] if len(ranked_data['domains']) > 3 else ranked_data['domains']\n",
    "    }\n",
    "    print(json.dumps(preview_ranked, ensure_ascii=False, indent=4))\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    \n",
    "    # 4. TLDåˆ†çµ„æ ¼å¼é è¦½\n",
    "    tld_data = generator.generate_tld_grouped()\n",
    "    print(\"ğŸŒ 4. TLDåˆ†çµ„æ ¼å¼ (tld_grouped.json):\")\n",
    "    print(f\"  â””â”€ æŒ‰é ‚ç´šåŸŸååˆ†çµ„\")\n",
    "    print(\"  â””â”€ çµæ§‹é è¦½:\")\n",
    "    first_tld = list(tld_data['tld_groups'].keys())[0]\n",
    "    preview_tld = {\n",
    "        \"metadata\": tld_data['metadata'],\n",
    "        \"tld_groups\": {\n",
    "            first_tld: tld_data['tld_groups'][first_tld][:2] + [{\"...\": \"æ›´å¤šåŸŸå\"}] if len(tld_data['tld_groups'][first_tld]) > 2 else tld_data['tld_groups'][first_tld],\n",
    "            \"...\": \"æ›´å¤šTLDåˆ†çµ„\"\n",
    "        }\n",
    "    }\n",
    "    print(json.dumps(preview_tld, ensure_ascii=False, indent=4))\n",
    "\n",
    "# é‹è¡Œé è¦½\n",
    "if 'domain_data' in locals() and domain_data:\n",
    "    preview_json_formats()\n",
    "    \n",
    "    print(f\"\\nğŸ“ ç”Ÿæˆçš„JSONæ–‡ä»¶ç”¨é€”èªªæ˜:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"1ï¸âƒ£  simple_list.json:\")\n",
    "    print(\"   â””â”€ ç´”åŸŸååˆ—è¡¨ï¼Œé©åˆå¿«é€ŸæŸ¥çœ‹å’Œå°å…¥å…¶ä»–å·¥å…·\")\n",
    "    print(\"2ï¸âƒ£  detailed_stats.json:\")\n",
    "    print(\"   â””â”€ å®Œæ•´çµ±è¨ˆä¿¡æ¯ï¼ŒåŒ…å«URLç¤ºä¾‹ã€å‡ºç¾æ¬¡æ•¸ç­‰\")\n",
    "    print(\"3ï¸âƒ£  frequency_ranked.json:\")\n",
    "    print(\"   â””â”€ æŒ‰ç†±é–€ç¨‹åº¦æ’åºï¼Œæ–¹ä¾¿è­˜åˆ¥ä¸»è¦åŸŸåä¾†æº\")\n",
    "    print(\"4ï¸âƒ£  tld_grouped.json:\")\n",
    "    print(\"   â””â”€ æŒ‰ç¶²åŸŸé¡å‹åˆ†çµ„ï¼Œä¾¿æ–¼åˆ†æåŸŸååˆ†å¸ƒç‰¹å¾µ\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ ä½¿ç”¨å»ºè­°:\")\n",
    "    print(\"  â€¢ ç”¨æ–¼ç™½åå–®/é»‘åå–®ç®¡ç†\")\n",
    "    print(\"  â€¢ åŸŸåä¿¡è­½åˆ†æ\") \n",
    "    print(\"  â€¢ æ•¸æ“šä¾†æºè¿½è¸ª\")\n",
    "    print(\"  â€¢ ç‰ˆæ¬Šé¢¨éšªè©•ä¼°\")\n",
    "    print(\"  â€¢ ç¶²ç«™åˆ†é¡å’Œéæ¿¾\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ è«‹å…ˆé‹è¡Œå‰é¢çš„æ•¸æ“šåŠ è¼‰å’Œè™•ç†æ­¥é©Ÿ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sms_cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
